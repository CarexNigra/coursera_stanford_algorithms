{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quicksort\n",
    "\n",
    "* competitive with, and often superior to, Merge Sort\n",
    "* suitable randomized implementation runs in time nlog(n) on average\n",
    "* constants hidden in the Big-O notation are extremely small\n",
    "* it operates in place: \n",
    "     * needs very little additional storage, beyond what's given in the input array\n",
    "     * does just repeated swaps within the space of the input array, until it finally concludes with a sorted version of the given array\n",
    "\n",
    "### 5.1 Ingredients\n",
    "* Sorting problem\n",
    "    * input an array of n numbers in arbitrary order\n",
    "    * Output: array in increasin order\n",
    "    * for the sake of simplicity: the input has no duplicates (distinct entries)\n",
    "        * Exercise: extend Quicksort to handle duplicates          \n",
    "        \n",
    "Key Subroutine: **partitioning array around a pivot element**\n",
    "* The Idea\n",
    "    * pick one element in your array to act as a pivot element\n",
    "        * eventually we'll worry quite a bit about exactly how we choose it. But for now you can just think of it that we pluck out the very first element in the array to act as the pivot\n",
    "    *  re-arrange an array in a way that \n",
    "        * all the elements which come to the left of the pivot element are less than the pivot, \n",
    "        * and all the elements which come after the pivot element are greater than the pivot\n",
    "        * we do not insist on correct order here\n",
    "* pivot element itself winds up in its rightful position\n",
    "* Partitioning:\n",
    "    * can be done quickly. \n",
    "    * It can be done in linear time. \n",
    "    * it's a way of making progress toward having a sorted version of an array. \n",
    "    * And it's gonna enable a divide-and-conquer approach toward sorting the input array\n",
    "* 2 cool facts:\n",
    "    * it it can be implemented in linear O(N) time,\n",
    "        * not just linear time but linear time with essentially no extra overhead. \n",
    "        * So we're gonna get a linear time of mutation, where all you do is repeated swaps. \n",
    "        * You do not allocate any additional memory. \n",
    "        * And that's key to the practical performance of the QuickSort algorithm.\n",
    "     * Secondly, it cuts down the problem size, so it enables the divide-and-conquer approach.\n",
    "         * recursively sort the elements that lie on the left of the pivot, \n",
    "         * and recursively sort the elements that lie on the right of the pivot.\n",
    "\n",
    "High-level description:  \n",
    "* Array A of length N \n",
    "* if n = 1 return A\n",
    "* NB: The big difference from Merge Sort is that, \n",
    "    * whereas in Merge Sort, we first split the array into pieces, recourse, and then combine the results, \n",
    "    * here, the recursive calls come last. \n",
    "* p = choose a Pivot element (A,n) [currently out of discussion, unimplimented]\n",
    "* Partition A around P\n",
    "* recurseviely sort 1st part\n",
    "* recursevely sort 2d part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Partitioning around a Pivot\n",
    "* in linear tima and in place!\n",
    "* if we didn't care about the in-place requirement, if we were happy to just allocate a second array and copy stuff over, it would actually be pretty easy to implement a Partition subroutine in linear time.\n",
    "    * Using O(N) extra memory, it's easy to partition around a pivot element in O(N) time.\n",
    "    * Theta(N) would be also correct, but I'm just going to write the weaker but still correct statement\n",
    "    * So O(N) time using linear extra memory. \n",
    "        * Well, if we're allowed to use linear extra space, we can just preallocate another array of length N.\n",
    "        * Then we can just do a simple scan through the input array, bucketing elements according to whether they are bigger than or less than the pivot (from both ends of a new array) \n",
    "* But if no additional space available\n",
    "    * case where the pivot is in fact the first element, but it works for any pivot\n",
    "        * you just implement a preprocessing step, which swaps it with the first element = O(cst) time\n",
    "    * The idea is, we're gonna be able to able to get away with just a single linear scan of the input array (single for loop) \n",
    "        * we'll be keeping track of both the part of the array we've looked at so far, and the part that we haven't looked at so far. \n",
    "        * the group we've seen gonna be split further, according to the elements that are less than the pivot and those that are bigger than the pivot = invariant\n",
    "        * So we're gonna leave the pivot element just hanging out in the first element of the array until the very end of the algorithm, when we correct its position with a swap. \n",
    "        * [p][   < p   ][   >p   ][   unseen   ]\n",
    "        * we need to keep track of 2 boundaries\n",
    "            * j – seen/unseen boundary\n",
    "            * i - less/more than p boundary\n",
    "        * in the begining i = j\n",
    "        * we want to make sure that at each step we advance \"j\", we look at one new element => linear number of steps for a scan\n",
    "        * if 2 and 3 elements not in rigt order – swap => i+, j+, \n",
    "        * if not => only j+\n",
    "        * if 4th element bigger than p => everything \"seen\" is still partitioned => j+\n",
    "        * if less than p => swap with the leftmost array entry which is bigger than p => i+, j+\n",
    "        * p element swaping with rightmost element which is smaller then the pivot\n",
    "       \n",
    "**Algorithm**\n",
    "* Partitioning(A, l, r) # l,r – needed to support recursion, denote the left and right boundaries of a subarray [input = A[l...r]]\n",
    "    * p = A[l]\n",
    "    * i = l+1\n",
    "    * for j = l+1 to r\n",
    "        * if A[j] < p   [if A[j] > p, do nothing]\n",
    "            * swap A[j] and A[i] [if you haven't yet seen any bigger elemens, you need not to swap]\n",
    "            * i++\n",
    "    * swap A[l] and A[i-1]\n",
    "    \n",
    "* There are different other approaches to partition\n",
    "* Running time O(n), where n = r-l+1 = length of input\n",
    "* O(1) work per array entry\n",
    "* Works in place: repeated swaps\n",
    "\n",
    "* NB: l = 0, r = n-1 needed to support recursion and work within the initial array. \n",
    "    * We won't always be partitioning the entire list. \n",
    "    * As we work through the sorting algorithm later, we are going to be working on smaller and smaller sublists, \n",
    "    * but because we don't want to create new copies of the list, we'll be defining these sublists by using indexes to the original list.\n",
    "\n",
    "\n",
    "**Correctness by induciton**   \n",
    "Claim: the for loop maintains the invariant\n",
    "* A[l+1], ..., A[i-1] are less than the pivot\n",
    "* A[i], ..., A[j-1] are greater than pivot\n",
    "Consequence: at the end of the loop we have a desired situation: [p][   < p   ][   > p   ]\n",
    "\n",
    "The correctness proofs for most of the other divide-and-conquer algorithms that we discuss can be formalized in a similar way\n",
    "\n",
    "*Induction*  \n",
    "* Let P(n) = assertion parameterized by positive integers n\n",
    "* For us: P(n) is \"QuickSort correctly sorts every input array of length n\"\n",
    "* How to prove P(n) for all n >= 1 by induction\n",
    "    * [base case] directly prove that P(1) holds\n",
    "    * [inductive step]: for every n >= 2 prove that \n",
    "        * if [P(k) holds for all k < n], then P(n) holds as well (iductive hypothesis in [])\n",
    "        * So for any given N that you care about, the way you can derive that from one and two is you just start from the base case, P of one holds. Then you apply the inductive step N minus one times.\n",
    "        \n",
    "*Correctness of QuickSort* \n",
    "* P(n) is \"QuickSort correctly sorts every input array of length n\"\n",
    "* Claim: P(n) holds for every n >= 1 [no matter how you choose the pivot]\n",
    "* Proof by induction:\n",
    "    * [base case] every input array of length 1 is already sorted. QS retyrns the input array which is correct (P(1) holds)\n",
    "    * [inductive step] Fix n > = 2. Fix some input array A of length n. Need to show: if [P(k) holds for every k =< n], then P(n) holds as well (iductive hypothesis in [])\n",
    "        * we're assuming that quicksort never makes a mistake on any input array that has length strictly smaller than n. And now we just have to show it never makes a mistake on array, input arrays that have size exactly n\n",
    "        * Recall: QuickSort first partitions around some pivot p\n",
    "        * Note: pivot winds up in correct position: [   < p   ][p][   > p   ]\n",
    "        * let k1, k2 = lengths за 1st and 2d parts of partitioned array\n",
    "        * Note: k1 and k2 < n because the pivot is excluded from both of those 2 parts\n",
    "        * By inductive hypothesis: 1st and 2d part are sorted correctly by recursive calls = P(k1) and P(k2) are correct because k1 and k2 < n\n",
    "        * So after recursive calls, entire array correctly sorted\n",
    "            * the first recursive call puts all of the elements that are less than the pivot in the correct relative order. \n",
    "            * Next comes the pivot, which is bigger than all of that stuff in the first part and less than all the stuff in the second part, \n",
    "            * and then the second recursive call correctly orders all of the elements in the second part. \n",
    "            * So with those three things pasted together, we have a sorted version of the input array and since this array was an arbitrary one, of link N. \n",
    "            * That establishes the assertion P of N and since n was arbitrary, that establishes the inductive      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Choosing good pivot\n",
    "\n",
    "* we are not currently in a position to discuss the running time of the QuickSort algorithm. \n",
    "* The reason is the running time of QuickSort depends crucially on how you choose the pivot. \n",
    "    * a pivot is good if it splits the partitioned array into roughly two equal sized subproblems. \n",
    "    * And it's bad, it's of low quality, if we get very unbalanced subproblems\n",
    "* what is the running time of this recursive QuickSort algorithm on an already sorted array, if we always use the first element of a subarray as the pivot?\n",
    "    * The answer is Theta(n^2) because for each partition (from n, n-1, to 1) the Subroutine does (n, n-1, to 1) operations respectively. \n",
    "    * So the total number is =< than n + (n-1) + ... + 1 = n(n-1)/2 = Theta(n^2)\n",
    "* What is the best case?\n",
    "    * ideally, we choose a pivot which gave us two sub-problems, both of size n over 2 or less.\n",
    "    * It's the median – exactly half of the elements are less than it and half of the elements are bigger than it\n",
    "    * Theta(n log(n))\n",
    "    * T(n) =< 2T(n/2) + O(n) (n/2 because of our choice of p; O(n) because of linear time of partition subroutine)\n",
    "    * = > T(n) = O(nlogn)\n",
    "    \n",
    "**How to choose pivots?**  \n",
    "* Big idea: random pivots\n",
    "    * Among the K candidate pivot elements in the sub-array, we're going to choose each one with probability of one over k.\n",
    "* Hope: a random pivot is \"pretty good\" \"often enough\"\n",
    "* Intuition: \n",
    "    * (1) if always get a 25-75 split, good enough for O(nlogn) running time [non trivial: prove via recursion tree]\n",
    "    * (2) if we pick p as anything between [26 and 75], the partitioning is fine = half of elements = flipping coins = 50% of probability of good split\n",
    "* Does this really work?\n",
    "    * QuickSort theorem: for every input array of length n, the average running time of Quick Sort with random pivots is O(nlogn)\n",
    "        * this is a worst case guarantee with respect to the input: for every input array of length n = we have absolutely no assumptions about the data.\n",
    "        * => holds for every input\n",
    "    * The averaging is coming only from randomness which is internal to our algorithm. Randomness that we put in the code in ourselves, that we're responsible for.\n",
    "    * So what this theorem is telling us is that for every possible input array, while the running time does indeed fluctuate between an upper bound of n squared and a lower bound of n log n. \n",
    "        * The best case is dominating. \n",
    "        * On average it's n log n, on average it's almost as good as the best case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Probability review\n",
    "\n",
    "* **Sample space** = Omega = all possible outcomes = in algorithms usually finite\n",
    "* **Probability** of each outcome i belonging to Omega = p(i); sum of p(i) on Omega = 1\n",
    "* **Event** = a subset S of Omega. Probability of event = Sum of probabilities of each outcome\n",
    "\n",
    "**Example of pivot**\n",
    "* S = {(n/4 + 1)th smallest element, ..., (3n/4)th smallest element}\n",
    "* Cardinality of S = 1/2\n",
    "* Pr[S] = [n/2]/n =1/2\n",
    "\n",
    "* **Random variable** X is a real valued function X: Omega => R\n",
    "    * 2 dice: Sum of 2 dice\n",
    "    * Pivot: Size of subarray passed to the 1st recursive call \n",
    "* **Expectation of a Random var** E[X] = average value of X = sum(X(i))*p(i)) over Omega\n",
    "    * Expectation: sum of two dice: brute force, pairing\n",
    "    * Expectation of the sizw of the subarrray passed to the first recursive call of QuickSort\n",
    "        * if X = subarray size\n",
    "        * then E[X] = 0*1/n + 1*1/n + 2*1/n + (n-1)*1/n = (n-1)/2\n",
    "\n",
    "**Linearity of Expectation**  \n",
    "* Claim [LIN EXP]: Let X1, ..., Xn be random variables defined on Omega\n",
    "    * Then E[Sum(X(i))] = Sum(E[X(i)]) \n",
    "    * Holds even when Xj are not independent!\n",
    "    * False if replace sums by products\n",
    "* Ex1: if X1, X2 = the 2 dice\n",
    "    * Then E[Xj] = 1/6 * (1+2+...+6) = 3.5\n",
    "    * By linearity: E[X1 + X2] = E[X1] + E[X2] = 3.5 + 3.5 = 7\n",
    "* Proof:\n",
    "    * sum_j_1_n[E[Xj]] = sum_j_1_n[sum_i_on_omega[Xj(i)*p(i)]] \n",
    "        * = sum_i_on_omega[sum_j_1_n[Xj(i)*p(i)]] \n",
    "        * = sum_i_on_omega[p(i)* sum_j_1_n[Xj(i)]]\n",
    "        * = E[sum_j_1_n[Xj]]\n",
    "        * QED\n",
    "\n",
    "**Load balancing**\n",
    "* Problem: need to assign n processes to n servers \n",
    "* Proposed solution: assign each process to a random server\n",
    "* Question: what is expected number of processes assigned to a server?\n",
    "    * Sample space Omega = all n^n possible assignments of processes to server, each equally likely\n",
    "    * Let Y = total number of processes assigned to a first server\n",
    "    * Goal: compute E[Y]\n",
    "    * Brute force computation over the n^n outcomes is a lot!\n",
    "    * Instead of focusing on the sum of Y, we focus on constituent part of Y \n",
    "        * For a given process j we define Xj = 1 if j-th process is assigned to a server 1; = 0 if not\n",
    "        * This is called indicator variable = indicates if or not certain event occurs\n",
    "        * The total number of processes Y that gets assigned to the first server is simply the sum, j equal 1 to n of Xj,\n",
    "    * E[Y] = E[sum_j_1_n[Xj]]\n",
    "        * = sum_j_1_n[E[Xj]]\n",
    "        * = sum_j_1_n(Pr[Xj=0]*0 + Pr[Xj=1]*1)\n",
    "        * = sum_j_1_n[1/n] \n",
    "        * = 1\n",
    "* Often we can get away with really simple heuristics just by making random choices\n",
    "\n",
    "**Conditional probability**  \n",
    "* Let X and Y belon to Omega\n",
    "    * They can be disjoint or intersect\n",
    "    * Then Pr[X|Y] = probability X given Y = Pr[X intersect Y]/Pr[Y]\n",
    "\n",
    "**Independence of 2 events**\n",
    "* X, Y of Omega are independent <=>  Pr[X intersect Y] = Pr[X]*Pr[Y]\n",
    "* this holds <=> Pr[X|Y] = Pr[X] <=> Pr[Y|X] = Pr[Y]\n",
    "* Warning: your intuition can be very wrong\n",
    "    * If things are independent by construction, then you might proceed, otherwise you should check!\n",
    "* Definition: random variables A,B on Omega are independent <=> Pr[A=a], Pr[B=b] are independent for all a,b\n",
    "    * <=> Pr[A=a and B=b] = Pr[A=a]*Pr[B=b]\n",
    "* Claim: for independent random variables A,B E[A*B] = E[A]*E[B]\n",
    "    * Proof: E[A*B] = sum_a,b[(a*b)*Pr[A=a and B=b]]\n",
    "        * = sum_a,b[(a*b)*Pr[A=a]*Pr[B=b]] since independencr\n",
    "        * = Sum_a[a*Pr[A=a]]* sum_b[b*Pr[B=b]]\n",
    "        * = E[A]*E[B]\n",
    "* Example: X1, X2 belong to [0, 1] be random, X3 = X1 xor X2 [exor = exclusive or = if both are 0 or 1, then the output is 0, if only one zero, the output is one]\n",
    "    * Omega = {000, 101, 011, 110}, each eaqually likely\n",
    "    * Claim: X1 and X3 are independent = we see all 4 combinations of X1 and X3 occur in Omega\n",
    "    * Claim: X1*X3 and X2 are not independent random variables\n",
    "        * Proof: sufficient to show that E[X1*X3*X2] ≠ E[X1*X3]*E[X2]\n",
    "            * E[X1*X3] = E[X1]*E[X3] = 1/4 since independence\n",
    "            * E[X2] = 1/2\n",
    "            * E[X1*X3*X2] = for any outcome in the samplespace the produvt of variables = 0\n",
    "            * => 0 = 1/8 false! \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Quicksort analysis\n",
    "\n",
    "Theorem: For every possible input array of length n, the average running time of QuickSort algorithm (with random pivots) is O(nlogn).\n",
    "* holds for every input [no assumptions on data]\n",
    "\n",
    "* fix input array A of length n\n",
    "* sample space Omega = all possible outcomes of random choises in QuickSort (i.e pivot sequences)\n",
    "* Key random variable: \n",
    "    * for a random point in the sample space = random pivot sequence = sigma in Omega\n",
    "    * C(sigma) = a number of comparisons between two different input elements QuickSort makes (given random choises sigma)\n",
    "* Lemma: running timer of QuickSort dominated by comparaisons, all other neigligible\n",
    "    * There is a constant c, such as for any sigma in Omega, RunningTime(sigma) =< c*C(sigma)\n",
    "    * Proof in notes\n",
    "* Remaining goal expectation: E[C] = O(nlogn)\n",
    "    * It's very hard to understand what this capital C is, it's fluctuating between nlogn and n^2. \n",
    "    * And it's hard to know how to get a handle on it.\n",
    "* Master method doesn't apply\n",
    "    * The size of 2 subproblems is random\n",
    "    * And subproblems are not balanced = different sizes of inputs\n",
    "    * It is possible to develop a theory of recurrence relations for randomized algorithms and apply that to QuickSort in particular. \n",
    "        * But it's really quite messy.\n",
    "        * And there is another approach:\n",
    "\n",
    "**Decomposition principle**\n",
    "* you take a random variable that's complicated, but that you care about a lot. \n",
    "    * You decompose it into simple random variables, which you don't really care about in their own, though it's easy analyze. \n",
    "    * And then you stitch those two things together using linearity and expectation\n",
    "* Building blocks:\n",
    "    * A = fixed input array \n",
    "    * Zi = i-th smallest element of A (i ≠ position, Zi can lay wherever!)\n",
    "    * For sigma in Omega, indicies i<j, let Xij(sigma) = # of times Zi,Zj get compared in full QuickSort execution with pivot sequence sigma\n",
    "        * Xij(sigma) = [0, 1]\n",
    "        * Not possible > 1: \n",
    "            * the only place in the code where comparisons between pairs of input array elements happens is the partition subroutine\n",
    "            * 2 elements are compared only if one is the pivot\n",
    "            * the pivot element is compared to each one element in the array once\n",
    "            * the pivot is excluded from future partition recursios\n",
    "            * Thus each Xij is an indicator random variable: indicates whether 2 elements get compared\n",
    "  \n",
    "* Principle:\n",
    "    * C(sigma) = a total number of comparisons between two different input elements QuickSort makes (given random choises sigma)\n",
    "     * Xij(sigma) = a number of times two particular elements Zi,Zj get compared in whole QuickSort execution with pivot sequence sigma\n",
    "     * Thus for any sigma, C(sigma) = sum_i_1_n-1 [sum_j_i+1_n [Xij(sigma)]]\n",
    "     * Linearity of expectation: \n",
    "         * for any random variables (even dependent): \n",
    "         * E[C] = sum_i_1_n-1 [sum_j_i+1_n E[Xij(sigma)]]\n",
    "         * E[C] – complicated, E[Xij] – simple: 0 or 1\n",
    "         * Since E[Xij] = 0*Pr[Xij=0] + 1*Pr[Xij=1] = Pr[Xij=1] \n",
    "         * Thus E[C] = sum_i_1_n-1 [sum_j_i+1_n [Pr[Xij=1]]\n",
    "         * = sum_i_1_n-1 [sum_j_i+1_n [Pr[Zi and Zj get compared]] (*)  \n",
    "\n",
    "* General decomposition pricniple\n",
    "    * Identify random variable Y that you really care about\n",
    "    * Express Y as a sum os simple (ideally indicator) random variables: Y = sum_i_1_n[Xi]\n",
    "    * Apply linearity of expectation: E[Y] = Sum_i_1_n [Pr[Xi=1]] \n",
    "\n",
    "* Exact expression for Pr[Zi and Zj get compared]\n",
    "    * Key claim: For any i<j, Pr[Zi and Zj get compared] = 2/(j-i+1)\n",
    "    * Proof: \n",
    "        * fix Zi and Zj with i<j\n",
    "        * Consider the set Zi, Zi+1, ..., Zj-1, Zj (they are not arranged in the imput array)\n",
    "        * Suppose none of them is chosen as a pivot, where the pivot can lie? eiter greater, or less then the set\n",
    "        * Inductively: as long as none of these are chosen as a pivot, all are passed to the same recursive call\n",
    "        * At some point one of them would be chosen as a pivot\n",
    "        * Case 1: the pivot either Zi or Zj: they will be definitely compared\n",
    "        * Case 2: if the pivot is niether Zi or Zj, they won't be compared now, neither they would be compared in future (because they would be split by pivot on 2 different recursive calls)\n",
    "        * Note: since pivots always chosen uniformly at randon, each of Zi, Zi+1, ..., Zj-1, Zj is equelly likely to be the first\n",
    "        * 2 cases out of j-i+1 that they get compared; j-i-1 cases that they won't\n",
    "        * Pr[Zi and Zj get compared] = 2/(j-i+1) \n",
    "        * QED\n",
    "\n",
    "* E[C] = 2* sum_i_1_n-1 [sum_j_i+1_n [1/(j-i+1)]] (still need to show that it's O(nlogn)\n",
    "    * the biggest sum gets when j and i are next to each other (1/2) and quadratic number of terms = O(n^2)\n",
    "    * we fix i and ask how big the inner sum could be.\n",
    "    * for each fixed i, the inner sum is sum_j_i+1_n[1/(j-i+1] = 1/2 + 1/3 + ... + 1/n\n",
    "    * E[C] =< 2 * (n-1 choices for i) * sum_k_2_n[1/k] (The biggest of those inner sums is the one occurring when I equals one)\n",
    "    * E[C] =< 2 * n * sum_k_2_n[1/k]\n",
    "    * Claim: sum_k_2_n[1/k] =< ln(n)\n",
    "    * sum_k_2_n[1/k] =< int_1_n[1/x dx] = ln(x)[1, n] = ln(n)-ln(1) = ln(n)\n",
    "    * QED\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 \n",
    "\n",
    "**1. Let 0<α<0.5 be some constant (independent of the input array length n). Recall the Partition subroutine employed by the QuickSort algorithm, as explained in lecture. What is the probability that, with a randomly chosen pivot element, the Partition subroutine produces a split in which the size of the smaller of the two subarrays is ≥α times the size of the original array?**\n",
    "\n",
    "* Array A of length n\n",
    "* Array B – smaller: Pr[len[B] >= α * n] = ?\n",
    "    * Pr[pivot = Xi] = 1/n\n",
    "    * Pr[len[B] >= α * n] = (n/2 - α * n) / (n/2) = 1-2α"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Now assume that you achieve the approximately balanced splits above in every recursive call --- that is, assume that whenever a recursive call is given an array of length k, then each of its two recursive calls is passed a subarray with length between αk and (1−α)k (where α is a fixed constant strictly between 0 and 0.5). How many recursive calls can occur before you hit the base case? Equivalently, which levels of the recursion tree can contain leaves? Express your answer as a range of possible numbers d, from the minimum to the maximum number of recursive calls that might be needed.**  \n",
    "\n",
    "* arr len k = [   A    ][p][      B     ]\n",
    "* arr len k = [        ]αk[   ]k/2[   ]k(1-α)[        ]\n",
    "* p must be in set [Xαk; Xk(1-α)] to make\n",
    "    * αk =< len A =< k(1-α) and\n",
    "    * αk =< len B =< k(1-α)\n",
    "* Each time we split a subarray in 2 unequel parts\n",
    "    * the smallest is of length k*(1/α) = defines min number of splits\n",
    "    * the biggest is of length k*(1/(1-α)) = defines max number of splits\n",
    "* we split smallest subarray m times till we get 1: k*(α)^m = 1 \n",
    "    * => m = min number of steps = log_α(1/k)\n",
    "        * = log_α(1) - log_α(k)\n",
    "        * = - log_α(k)\n",
    "        * = - log(k)/log(α)\n",
    "* we split bigest subarray n times till we get 1: k*(1-α)^n = 1 \n",
    "    * => n = max number of steps = log_1/(1-α)(1/k)\n",
    "        * = log_(1-α)(1) - log_(1-α)(k)\n",
    "        * = - log_(1-α)(k)\n",
    "        * = - log(k)/log(1-α)       \n",
    "* -log(k)/log(α) =< d =< -log(k)/log(1-α) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Define the recursion depth of QuickSort to be the maximum number of successive recursive calls before it hits the base case --- equivalently, the number of the last level of the corresponding recursion tree. Note that the recursion depth is a random variable, which depends on which pivots get chosen. What is the minimum-possible and maximum-possible recursion depth of QuickSort, respectively?**  \n",
    "\n",
    "* Minimum if p is always taken in the middle of the array values = log_2(n) recursions\n",
    "* Maximum if p is always taken on one of array ends = n-1 recursions\n",
    "* => Theta(logn); Theta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Consider a group of k people. Assume that each person's birthday is drawn uniformly at random from the 365 possibilities. (And ignore leap years.) What is the smallest value of k such that the expected number of pairs of distinct people with the same birthday is at least one? [Hint: define an indicator random variable for each ordered pair of people. Use linearity of expectation.]**\n",
    "\n",
    "* Sample space Omega = all 365^k possible birthday combinations of k people\n",
    "* Zij = indicator variable of person i and person j having birthday at the same day = 1 if yes, 0 if no\n",
    "* Y = total number of paris where i<j: Xi=Xj\n",
    "* Y = sum_i_1_k-1 [sum_j_i+1_k [Zij]]\n",
    "* E[Y] = sum_i_1_k-1 [sum_j_i+1_k [E[Zij]]]\n",
    "    * since E[Zij] = 0*Pr[Zij = 0] + 1*Pr[Zij = 1]\n",
    "    * E[Y] = sum_i_1_k-1 [sum_j_i+1_k [Pr[Zij = 1]]]\n",
    "        * Pr[Zij = 1] = number of cases when [Xi = a and Xj = a] / All cases \n",
    "        * = 365 / 365^2 = 1/365 \n",
    "    * E[Y] = sum_i_1_k-1 [sum_j_i+1_k [1/365]]]\n",
    "        * = 1/365* sum_i_1_k-1 [k-i-1]\n",
    "        * = 1/365* [(k-1)*(k-1) - (k-1)*k/2]\n",
    "        * = 1/365* [(k^2)/2 - 3k/2 + 1]\n",
    "        * E[Y] >= 1\n",
    "    * (k^2)/2 - 3k/2 + 1 >= 365\n",
    "        * k^2 - 3k + 2 - 365*2 >= 0\n",
    "        * k^2 - 3k - 728 >= 0\n",
    "        * k = (3 + 54)/2 = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Let X_1, X_2, X_3 denote the outcomes of three rolls of a six-sided die. (I.e., each X_i is uniformly distributed among {1,2,3,4,5,6} and by assumption they are independent.) Let Y denote the product of X_1 and X_2 and Z the product of X_2 and X_3. Which of the following statements is correct?**\n",
    "\n",
    "1. Y and Z are independent, but E[Y*Z] ≠ E[Y]∗E[Z].\n",
    "2. Y and Z are not independent, but E[Y*Z] = E[Y]∗E[Z].\n",
    "3. Y and Z are independent, and E[Y*Z] = E[Y]∗E[Z].\n",
    "4. Y and Z are not independent, and E[Y*Z] ≠ E[Y]∗E[Z].\n",
    "\n",
    "* {X_1, X_2, X_3}\n",
    "* X_i = {1, ..., 6}\n",
    "* Y = X_1*X_2 \n",
    "* Z = X_2*X_3\n",
    "\n",
    "* E[Y] = avg value of Y\n",
    "* E[Z] = avg value of Z\n",
    "* when Y is calculated, it defines X2, which influences Z => Y and Z might be dependent\n",
    "\n",
    "\n",
    "* E[Y*Z] = E[X1 * (X2)^2 * X3] among different ways, it can surely be split into:  \n",
    "    * = E[X1] * E[(X2)^2] * E[X3] since Xi are independent\n",
    "* E[Y]∗E[Z] = E[X1*X2]∗E[X2*X3]\n",
    "    * = E[X1] *(E[X2])^2 ∗ E[X3]\n",
    "* We need to compare E[(X2)^2] and (E[X2])^2\n",
    "    * \n",
    "\n",
    "    * \n",
    "    * => E[Y*Z] ≠ E[Y]∗E[Z] \n",
    "    * => (4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Assignment 3\n",
    "\n",
    "**Exercise 1**  \n",
    "The file contains all of the integers between 1 and 10,000 (inclusive, with no repeats) in unsorted order. The integer in the i-th row of the file gives you the i-th entry of an input array.\n",
    "\n",
    "Your task is to compute the total number of comparisons used to sort the given input file by QuickSort. As you know, the number of comparisons depends on which elements are chosen as pivots, so we'll ask you to explore three different pivoting rules.\n",
    "\n",
    "You should not count comparisons one-by-one. Rather, when there is a recursive call on a subarray of length m, you should simply add m−1 to your running total of comparisons. (This is because the pivot element is compared to each of the other m−1 elements in the subarray in this recursive call.)\n",
    "\n",
    "WARNING:  \n",
    "The Partition subroutine can be implemented in several different ways, and different implementations can give you differing numbers of comparisons. For this problem, you should implement the Partition subroutine exactly as it is described in the video lectures (otherwise you might get the wrong answer).\n",
    "\n",
    "DIRECTIONS FOR THIS PROBLEM:  \n",
    "For the first part of the programming assignment, you should always use the first element of the array as the pivot element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2148, 9058, 7742, 3153, 6324, 609, 7628, 5469, 7017, 504]"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../algorithms_course_code/data/pg_asmt_3_data_quicksort.txt', 'r') as f:\n",
    "    x = f.read().splitlines()\n",
    "    f.close()\n",
    "int_list = [int(i) for i in x]\n",
    "int_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my version\n",
    "# works only 1 time, then \"total\" should be reloaded to 0\n",
    "\n",
    "def _Partition1(arr, l, r, comp_num):  \n",
    "    pivot = arr[l]\n",
    "    i = l + 1 # index of the first element of bigger values part \n",
    "    comp_num = len(arr[l:r]) # number of comparisons \n",
    "    \n",
    "    for j in range (l+1, r+1): # index of a first unseen element\n",
    "        if arr[j] < pivot: # Nb: if A[j] > p, do nothing      \n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "            i += 1       \n",
    "    arr[l], arr[i-1] = arr[i-1], arr[l]\n",
    "    # print(arr, comp_num)\n",
    "    return i-1, comp_num  # the final index of partition and number of comparisions per partition\n",
    "\n",
    "def _QuickSort1(arr, l, r, comp_num):    \n",
    "    global total\n",
    "    if l < r: \n",
    "        (partition_index, comp_num_step) = _Partition1(arr, l, r, comp_num)       \n",
    "        _QuickSort1(arr, l, partition_index-1, comp_num_step)\n",
    "        _QuickSort1(arr, partition_index+1, r, comp_num_step)\n",
    "        total += comp_num_step\n",
    "    return total\n",
    "    \n",
    "def QuickSort1(arr):      \n",
    "    total = _QuickSort1(arr, 0, len(arr)-1, 0)\n",
    "    return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 1, 3, 8, 55, 77, 14, 21] 8\n",
      "[3, 2, 1, 6, 8, 55, 77, 14, 21] 3\n",
      "[1, 2, 3, 6, 8, 55, 77, 14, 21] 2\n",
      "[1, 2, 3, 6, 8, 55, 77, 14, 21] 1\n",
      "[1, 2, 3, 6, 8, 21, 14, 55, 77] 3\n",
      "[1, 2, 3, 6, 8, 14, 21, 55, 77] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [8, 55, 2, 1, 21, 3, 77, 14, 6]\n",
    "total = 0\n",
    "QuickSort1(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner approach, no need in global variable\n",
    "\n",
    "def _Partition2(arr, l, r):  \n",
    "    pivot = arr[l]\n",
    "    i = l + 1 # index of the first element of bigger values part \n",
    "    comp_count = r-l # number of comparisons \n",
    "    \n",
    "    for j in range (l+1, r+1): # index of a first unseen element\n",
    "        if arr[j] < pivot: # Nb: if A[j] > p, do nothing      \n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "            i += 1       \n",
    "    arr[l], arr[i-1] = arr[i-1], arr[l]\n",
    "    \n",
    "    # print(\"Partition subroutine: \", arr, comp_count, \" l=\", l, \" r =\", r)\n",
    "    return i-1, comp_count  # the final index of partition and number of comparisions per partition\n",
    "\n",
    "def _QuickSort2(arr, l, r):\n",
    "    comp_count = 0\n",
    "    \n",
    "    if l < r:\n",
    "        (partition_index, comp_count) = _Partition2(arr, l, r)\n",
    "        comp_count += _QuickSort2(arr, l, partition_index-1)\n",
    "        comp_count += _QuickSort2(arr, partition_index+1, r)\n",
    "    return comp_count\n",
    "\n",
    "    \n",
    "def QuickSort2(arr):      \n",
    "    return _QuickSort2(arr, 0, len(arr)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [8, 55, 2, 1, 21, 3, 77, 14, 6]\n",
    "l = 0\n",
    "r = len(arr)-1\n",
    "QuickSort2(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162085"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = int_list\n",
    "QuickSort2(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 25  y = 620  z = 11175\n"
     ]
    }
   ],
   "source": [
    "arr1 = int_list[0:10]\n",
    "arr2 = int_list[0:100]\n",
    "arr3 = int_list[0:1000]\n",
    "x = QuickSort2(arr1)\n",
    "y = QuickSort2(arr2)\n",
    "z = QuickSort2(arr3)\n",
    "print(\"x =\", x, \" y =\", y, \" z =\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[504, 609, 2148, 3153, 5469, 6324, 7017, 7628, 7742, 9058]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**  \n",
    "Compute the number of comparisons (as in Problem 1), always using the final element of the given array as the pivot element. Again, be sure to implement the Partition subroutine exactly as it is described in the video lectures.\n",
    "\n",
    "Recall from the lectures that, just before the main Partition subroutine, you should exchange the pivot element (i.e., the last element) with the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Partition3(arr, l, r):      \n",
    "    arr[l], arr[r] = arr[r], arr[l]\n",
    "    pivot = arr[l]\n",
    "    i = l + 1 # index of the first element of bigger values part \n",
    "    comp_count = r-l # number of comparisons \n",
    "    \n",
    "    for j in range (l+1, r+1): # index of a first unseen element\n",
    "        if arr[j] < pivot: # Nb: if A[j] > p, do nothing      \n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "            i += 1       \n",
    "    arr[l], arr[i-1] = arr[i-1], arr[l]\n",
    "    \n",
    "    # print(\"Partition subroutine: \", arr, comp_count, \" l=\", l, \" r =\", r)\n",
    "    return i-1, comp_count  # the final index of partition and number of comparisions per partition\n",
    "\n",
    "def _QuickSort3(arr, l, r):\n",
    "    comp_count = 0\n",
    "    \n",
    "    if l < r:\n",
    "        (partition_index, comp_count) = _Partition3(arr, l, r)\n",
    "        comp_count += _QuickSort3(arr, l, partition_index-1)\n",
    "        comp_count += _QuickSort3(arr, partition_index+1, r)\n",
    "    return comp_count\n",
    "   \n",
    "def QuickSort3(arr):      \n",
    "    return _QuickSort3(arr, 0, len(arr)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [8, 55, 2, 1, 21, 3, 77, 14, 6]\n",
    "QuickSort3(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164123"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = int_list\n",
    "QuickSort3(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 31  y = 573  z = 10957\n"
     ]
    }
   ],
   "source": [
    "arr1 = int_list[0:10]\n",
    "arr2 = int_list[0:100]\n",
    "arr3 = int_list[0:1000]\n",
    "x = QuickSort3(arr1)\n",
    "y = QuickSort3(arr2)\n",
    "z = QuickSort3(arr3)\n",
    "print(\"x =\", x, \" y =\", y, \" z =\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**  \n",
    "Compute the number of comparisons (as in Problem 1), using the \"median-of-three\" pivot rule. [The primary motivation behind this rule is to do a little bit of extra work to get much better performance on input arrays that are nearly sorted or reverse sorted.] In more detail, you should choose the pivot as follows. Consider the first, middle, and final elements of the given array. (If the array has odd length it should be clear what the \"middle\" element is; for an array with even length 2k, use the k-th element as the \"middle\" element. So for the array 4 5 6 7, the \"middle\" element is the second one ---- 5 and not 6!) Identify which of these three elements is the median (i.e., the one whose value is in between the other two), and use this as your pivot. As discussed in the first and second parts of this programming assignment, be sure to implement Partition exactly as described in the video lectures (including exchanging the pivot element with the first element just before the main Partition subroutine).\n",
    "\n",
    "EXAMPLE:  \n",
    "For the input array 8 2 4 5 7 1 you would consider the first (8), middle (4), and last (1) elements; since 4 is the median of the set {1,4,8}, you would use 4 as your pivot element.\n",
    "\n",
    "SUBTLE POINT:  \n",
    "A careful analysis would keep track of the comparisons made in identifying the median of the three candidate elements. You should NOT do this. That is, as in the previous two problems, you should simply add m-1 to your running total of comparisons every time you recurse on a subarray with length mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _FindPivotIndex(arr, l, r):\n",
    "    if (r-l+1)%2 == 0:\n",
    "        med = l + (r-l-1)//2        \n",
    "    else:\n",
    "        med = l + (r-l)//2\n",
    "        \n",
    "    candidates = [arr[l], arr[med], arr[r]]\n",
    "    candidates.sort()     \n",
    "    \n",
    "    if candidates[1] == arr[med]:\n",
    "        return med\n",
    "    elif candidates[1] == arr[l]:\n",
    "        return l\n",
    "    else:\n",
    "        return r\n",
    "    \n",
    "def _Partition4(arr, l, r):      \n",
    "    proto_pivot_index = _FindPivotIndex(arr, l, r) # find index of best pivot\n",
    "    \n",
    "    arr[l], arr[proto_pivot_index] = arr[proto_pivot_index], arr[l] # swap element under this index with the first one    \n",
    "    pivot = arr[l]\n",
    "    i = l + 1 # index of the first element of bigger values part \n",
    "    comp_count = r-l # number of comparisons \n",
    "    \n",
    "    for j in range (l+1, r+1): # index of a first unseen element\n",
    "        if arr[j] < pivot: # Nb: if A[j] > p, do nothing      \n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "            i += 1       \n",
    "    arr[l], arr[i-1] = arr[i-1], arr[l]\n",
    "    \n",
    "    # print(\"Partition subroutine: \", arr, comp_count, \" l=\", l, \" r =\", r)\n",
    "    return i-1, comp_count  # the final index of partition and number of comparisions per partition\n",
    "\n",
    "def _QuickSort4(arr, l, r):\n",
    "    comp_count = 0\n",
    "    \n",
    "    if l < r:\n",
    "        (partition_index, comp_count) = _Partition4(arr, l, r)\n",
    "        comp_count += _QuickSort4(arr, l, partition_index-1)\n",
    "        comp_count += _QuickSort4(arr, partition_index+1, r)\n",
    "    return comp_count\n",
    "   \n",
    "def QuickSort4(arr):      \n",
    "    return _QuickSort4(arr, 0, len(arr)-1)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 21  y = 502  z = 9735\n"
     ]
    }
   ],
   "source": [
    "arr1 = int_list[0:10]\n",
    "arr2 = int_list[0:100]\n",
    "arr3 = int_list[0:1000]\n",
    "x = QuickSort4(arr1)\n",
    "y = QuickSort4(arr2)\n",
    "z = QuickSort4(arr3)\n",
    "print(\"x =\", x, \" y =\", y, \" z =\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138382"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = int_list\n",
    "QuickSort4(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Linear Time Selection\n",
    "\n",
    "### 8.1 Randomized selection\n",
    "\n",
    "* Selection problem: computing ordered statistics of an array with computing the median of an array being a special case\n",
    "    * the goal is the design and analysis of a super practical randomized algorithm that solves the problem\n",
    "    * running time would be linear in the length of the input array = O(n)\n",
    "* Input: \n",
    "    * Array A of n distinct numbers\n",
    "    *  in addition, you're told what order statistic you're looking for\n",
    "        * that's going to be a number i, which is an integer between 1 and N. \n",
    "* Output just a single number: the i-th order statistic = the i-th smallest entry in this input array\n",
    "    * The first order statistic is the minimum element of the array. Easier to find with a linear scan. \n",
    "    * The n-th order statistic is the maximum, again easier, easy to find with a linear scan. \n",
    "    * The middle element is the median = canonical version of selection problem\n",
    "        * if n is odd: median is a middle element = (n+1)/2 -th order statistic \n",
    "        * if n is even: there's two possible medians, we take the smaller of them = n/2 -th order statistic\n",
    "\n",
    "Why you'd ever want to compute the median of an array rather than the mean, that is the average. \n",
    "* One motivation is it's a more robust version of the mean. \n",
    "    * So if you just have a data entry problem and it corrupts one element of an input array it can totally screw up the average value of the array, but it has generally very little impact on the median.  \n",
    "\n",
    "\n",
    "We assume that the array entries are distinct, that is there's no repeated elements. But algorithm can be adopted to process repeated elements as well.\n",
    "\n",
    "We already have a pretty good algorithm to solve selection problem in O(nlog(n)) time\n",
    "* (1) Sorting of input array: we pick MergeSort\n",
    "* (2) Return i-th element of sorted sorted array\n",
    "\n",
    "This is called reduction: solution of one problem by reducing it to another problem that we already know how to solve. **But we can do better**\n",
    "\n",
    "* Certainly we're going to have to look at all the elements in the input array, in the worst case. \n",
    "    * => You shouldn't expect to do better than linear, but hey, why not linear time?\n",
    "* Actually if you think about it, we probably should have asked that question back when we were studying the sorting problem. \n",
    "    * Why were we so content with \n",
    "        * the nlog(n) time bound for mergesort. \n",
    "        * O(nlog(n)) time on average bound, for quick sort. \n",
    "    * Well it turns out, we have a really good reason to be happy with our nlog(n) upper bounds for the sorting problem. This will be the subject of the optional video. \n",
    "    *  **You actually can't sort an input array of length N better than nlog(n) time**. Either in the worst case or an average.\n",
    "    * => reduction to the sorting problem would limit us to nlon(n) time as well (for comparison sorting algorithms)\n",
    "* And if we want to do better than nlog(n) for selection we have to do it using ingenuity beyond this reduction, we have to prove that selection is a strictly easier problem then sorting. \n",
    "    * It can be done in linear time\n",
    "    * And it will be a randomized algorithm \n",
    "    * Obtained by modifying QuickSort\n",
    "    \n",
    " \n",
    "* In sorting problem we have 2 types of algorithms of O(nlog(n)) time\n",
    "    * Randomized = QuickSort\n",
    "    * Not randomized = MergeSort\n",
    "* Is there a non-randomized analog for Selection algoritm that gives the same time (O(n))?\n",
    "    * Yes = deterministic\n",
    "    * Its key idea is to choose the pivot deterministically in a very careful way using a trick called the median of medians.\n",
    "    * But randomized is more practical since it has both smaller constants and works in place\n",
    "\n",
    "**Randomized Selection Algorithm**  \n",
    "We can modify the QuickSort paradigm in order to directly solve The selection problem\n",
    "* (1) Partition subroutine would be used here as well as first step (chose the pivot, partition)\n",
    "    * [    < p    ][p][   > p    ]\n",
    "* (2) but how we are going to recurse: how to find the i-th order statistic of the original input array? It suffices to recurse on just one sub problem of smaller size, and find a suitable or a statistic in it\n",
    "    * The order of the statistics should be compared to the index of pivot after partitioning\n",
    "        * if p index > i: the recursion should be done on left part, new order = old order\n",
    "        * if p index < i: the recursion should be done on right part, and the new order statistics is i-index of p\n",
    "        * if p index = i: we are done\n",
    "\n",
    "RSelect(array A, length n, order statistic i)\n",
    "* (0) if n = 1 return A[1]\n",
    "* (1) choose pivot p from A (uniformly at random)\n",
    "* (2) partition A around p\n",
    "    * let j = new index of p\n",
    "* (3) if j = i return p\n",
    "* (4) if j > i return RSelect(left part of A, j-1, i)\n",
    "* (5) else return RSelect(right part of A, n-j, i-j)\n",
    "\n",
    "**The claim**: RSelect is correct (guaranteed to output i-th order statistic)   \n",
    "Proof: by induction like in QuickSort case  \n",
    "Running time?: depends on \"quality\" of the chosen pivots\n",
    "* Worst case\n",
    "    * As for QuickSort: because for each partition (from n, n-1, to 1) the Subroutine does (n, n-1, to 1) operations respectively.\n",
    "    * So the total number is =< than n + (n-1) + ... + 1 = n(n-1)/2 = Theta(n^2)\n",
    "* Best case Key: find pivot giving balanced split\n",
    "    * It's the median \n",
    "        * this is not super-helpful, because the median might well be what we're looking for in the first place. \n",
    "        * So this is sort of a circular idea. \n",
    "        * But for intuition, it's still worth exploring what kind of running time we would get in the best case\n",
    "    * Would get recurrence T(n) =< T(n/2) + O(n) (O(n) = time of partitioning)\n",
    "        * T(n) = O(n) (using case 2 of master method)\n",
    "        * This is just for intuition, \n",
    "* The hope: random pivot is \"pretty good\" \"often enough\"\n",
    "\n",
    "**RSelect Theorem**: For every input array of lenght n, the average running time of RSelect is O(n)  \n",
    "* holds for every input: no assumption on data; \n",
    "* randomness is not in the data, it is in the code (from the random choice of pivot)\n",
    "\n",
    "**ANALYSIS**      \n",
    "Proof of RSelect Theorem  \n",
    "\n",
    "* The first thought that you might have, and this would be a good thought, would be that we should proceed exactly the same way that we did in Quick Sort. \n",
    "    * We set up these indicator random variables, x, i ,j \n",
    "    * determining whether or not a given, pair of elements got compared at any point in the algorithm. \n",
    "    * And then we just realized the sum of the comparisons is just the sum, overall, of these x,i, js. \n",
    "    * We applied linearity of expectation and it boiled down to just figuring out the probability that a given pair of elements gets compared. \n",
    "    * You can analyze this randomized selection algorithm in exactly the same way. And it does give you a linear time bound on average.  \n",
    "    * But it's a little messy. It winds up being not quite as clean as in the quick sort analysis\n",
    "\n",
    "**Proof pt. 1: Tracking progress via phases**\n",
    "* Note: all of the work that gets done outside of the recursion calls is just the partition, which is O(n) time\n",
    "    * <=> RSelect uses =< C*n opertations outside of the recursive call [C = const > 0]\n",
    "* Notation: RSelect is in \"phase j\" if current array size is between (3/4)^(j+1) * n and (3/4)^j * n\n",
    "    * j = 0: phase 0 operates on arrays with size n and 75% of n\n",
    "    * first, outermost recursive call operates on array size n, so it is of the pahse 0\n",
    "    * but depending on future selection of pivots you may or not to get out of phase 0\n",
    "    * the phase number J, quantifies the number of times we've made 75 percent progress, relative to the original input array\n",
    "* Notation: Xj = counts the number of recursive calls in which a randomized selection algorithm is in phase J.\n",
    "    * It could be zero for some phases\n",
    "    * it is an integer\n",
    "* in terms of these XJs, counting the number of iterations in each phase, we can derive a relatively simple upper bound on the number of operations that our algorithm requires.\n",
    "* Note: **running time of RSelect =< sum_over_all_phases_j [Xj * С * (3/4)^j * n]** (*)\n",
    "    * (3/4)^j * n – is an upperbound of the array size during phase j (by the def of the phase)\n",
    "    * we multiply it by C – the amoutn of work that we do on each phase j subproblem\n",
    "    * and finally we multiply it by a number of subproblems we have in phase о\n",
    "    * that gives us the amount of work in phase j\n",
    "    * Than we sum over all phases\n",
    "\n",
    "* We are analyzing the running time of a randomized algorithm, so running time of RSelect is a random variable\n",
    "    * Because its value depends on how pivot is randomly chosen\n",
    "    * The right side of inequality is also a random variable: because Xj's are random variables, depending also on a choice of pivots\n",
    "    * And we care about the expectations of these variables\n",
    "* We should understand the average value of an Xj (the expected value)\n",
    "\n",
    "**Proof pt. 2: Reduction to coin flipping**  \n",
    "* to analyze the expectation of XJ, it's sufficient to understand the expectation of a very simple coin flipping experiment\n",
    "\n",
    "Step 1:  \n",
    "* Xj = num of recursive calls during phase j\n",
    "* Note: if RSelect chooses a pivot giving a 25-75 split (or better), then current phase must end!\n",
    "    * <=> new subarray length at most 75% of old length\n",
    "* Recall: probability of 25-75 split (or better) is 50%\n",
    "    * Thus we can reduce our analysis of the number of recursive calls during a given phase, to a simple experiment involving flipping coins\n",
    "    * E[Xj] =< expected number of times you need to flip a fair coin to get first \"heads\"\n",
    "        * \"heads\" =  being you're in Phase J, and if you get a good pivot, it gives you a 25/75 split\n",
    "        * that guarantees that you exit this Phase J. \n",
    "        * Just like it guarantees that you get to terminate the coin flipping experience, experiment.\n",
    "    * This correspondence give us a very elementary way to think about the progress that, that our randomized selection algorithm is making. \n",
    "        * So, there's one recursive call in every step in our algorithm, and each time we either choose a good pivot or a bad pivot, both could happen, 50-50 probability. \n",
    "        * A good pivot means we get a 75-25 split or better. \n",
    "        * A bad pivot means, by definition, we get a split worse than 25-75. \n",
    "        * We've reduced the task of upper bounding the expected number of recursive calls in a phase J to understanding the expected number of times you have to flip a fair coin before you get one hit. \n",
    "\n",
    "Step 2:  \n",
    "* The classical and precise answer to this coin flipping experiment. \n",
    "* N = random variable, the number of coin flips you need to do before you see the first heads. \n",
    "    * These random variables have their own name = geometric random variable with parameter one-half. \n",
    "    * So you can use a few different methods to compute the expected value of a geometric random variable such as this, and brute force using the definition of expectation works fine as long as you know how to manipulate infinite sums. \n",
    "    * But another approach is to write to the expected value of this random variable in terms of itself and then solve  for the expectation. \n",
    "    * So how many coins flips do you need? \n",
    "        * Well for sure you're gonna need one. That's the best case scenario. \n",
    "        * Either you get heads and that has 50 percent probability you stop \n",
    "        * or you get tails that happens with 50 percent probability and now you start all over again. \n",
    "        * On average how many times does that take? \n",
    "        * Well by the definition of capital N you expect the expectation of N coin flips, in the case where you get tails, and you have to start all over. \n",
    "        * E[N] = 1 + E[N-1] (to get N flips till we get \"heads\" we need to flip at least once => 1; and we add the expectetion to get heads in N-1 flips)\n",
    "        * but E[N-1] = 1 * 1/(2^(N-1)) = 1/2 * (1/(2^N)) = 1/2 * E[N]\n",
    "        * => E[N] = 1 + 1/2 * E[N]\n",
    "        * E[N] = 2\n",
    "    * Recall: we showed that Xj is bounded above by the expected number of coin flips until the heads. E[Xj] =< E[N]\n",
    "        * So this exact calculation of two for the coin flips gives us an upper bound of 2 for the number of recursive calls on average in any given phase J. \n",
    "        \n",
    "        \n",
    "Putting all together:  \n",
    "* running time of RSelect =< sum_over_all_phases_j [Xj * С * (3/4)^j * n]\n",
    "    * =< C * n * sum_over_all_phases_j [Xj * (3/4)^j ]\n",
    "* E[running time of RSelect] =< E[C * n * sum_over_all_phases_j [Xj * (3/4)^j ]]\n",
    "    * = C * n * sum_over_all_phases_j [E[Xj] * (3/4)^j]\n",
    "    * =< C * n * sum_over_all_phases_j [E[number of coin flips N] * (3/4)^j]\n",
    "    * = C * n * sum_over_all_phases_j [2 * (3/4)^j]\n",
    "    * = 2 * C * n * sum_over_all_phases_j [(3/4)^j] (geometric sum)\n",
    "    * =< 2 * C * n * [1/(1-3/4)] \n",
    "    * = 2 * 4 * C * n = 8Cn\n",
    "    * = O(n)\n",
    "    * QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Deterministic Selection\n",
    "\n",
    "* Still runs in linear time, but now in the worst case (not on avarege as randomized) for every single input array\n",
    "* This deterministic algorithm will get the same running time O(n), as the RSelect algorithm does on average. \n",
    "    * That said, the algorithm we're gonna cover here is not as fast as RSelect in practice, both \n",
    "        * because the hidden constants in it are larger. \n",
    "        * and also because it doesn't' operate in place. \n",
    "* The deterministic algorithm covered here is a modification of the RSelect randomized algorithm\n",
    "* So now the big question is: suppose we weren't permitted to make use of randomizations. What could we do? \n",
    "    * A good pivot is one that gives us a balanced split, after we do the partitioning of the array. \n",
    "    * That is, we want as close to a 50/50 split between the first and the second parts of the partitioned array as possible\n",
    "    * perfect 50/50 split would be the median\n",
    "* we have to have some subroutine that deterministically finds us a pretty good approximation of the median. \n",
    "    * And the big idea in this linear time selection algorithm, is to use what's called the **median of medians** as a proxy for the true meaning of the input array\n",
    "\n",
    "**ChosePivot(A,n) subroutine**   \n",
    "the high-level strategy:\n",
    "* we're gonna think about the elements of this array like sports teams, \n",
    "* and we're gonna run a tournament, a 2-round. \n",
    "* Knockout tournament, and the winner of this tournament is going to be who we return as the proposed pivot element\n",
    "* Then we'll have to prove that this is a pretty good pivot element\n",
    "    * logically break A into n/5 groups of size 5 each\n",
    "    * sort each group (e.g using MergeSort)\n",
    "    * copy n/5 medians (ie middle elements of each sorted group) into array C\n",
    "    * recursively compute the median of C\n",
    "    * return this as pivot\n",
    "\n",
    "DSelect(array A, length n, order statistic i)\n",
    "* (0) if n = 1 return A[1]\n",
    "* Choose pivot subroutine (new!)\n",
    "    * (1) break A into groups of 5, sort each group \n",
    "    * (2) C = the n/5 \"middle elements\"\n",
    "    * (3) p = DSelect(C, n/5, n/10) (recursively computes median of C) \n",
    "* (4) partition A around p\n",
    "    * let j = new index of p\n",
    "* (5) if j = i return p\n",
    "* (6) if j > i return RSelect(left part of A, j-1, i)\n",
    "* (7) else return RSelect(right part of A, n-j, i-j)\n",
    "\n",
    "What is confusing:\n",
    "* There are 2 recursive calls\n",
    "* The role of those 2 recursive calls are different\n",
    "    * the second is standard devide and conquer\n",
    "    * the first is part of identifying a good pivot element for this outer recursive call \n",
    "        * and this is so counter-intuitive, m\n",
    "        * an impression that this algorithm will go into an infinite loop\n",
    "        * but if you only recurs on smaller sub-problems, you're definitely going to terminate.\n",
    "\n",
    "**Running time of DSelect** \n",
    "* this selection algorithm terminate, run in finite time, \n",
    "* and it actually runs in linear time. \n",
    "* No matter what the input array is\n",
    "DSelectTheorem: for every input array of length n, DSelect runs in O(n) time \n",
    "* Warning: not as good as RSelect in practice\n",
    "    * worse constants\n",
    "    * not in-place, so uses extra memory\n",
    "\n",
    "**[ANALYSIS]**\n",
    "* The asymptotic running time of step 1 of DSelect is O(n)\n",
    "    * Why not O(nlog(n)) since it is sorting?\n",
    "    * Since we are sorting only subarrays that have only 5 elements, that's not so hard, that can be done in constant time\n",
    "    * Note: sorting an array of 5 elements takes =< 120 operations \n",
    "        * Why 120? We counted that in our 6*m*(log_2(m) + 1)\n",
    "        * So n/5 * 120 = 24 n = O(n)  \n",
    "        \n",
    "* (0) if n = 1 return A[1] => **O(1)**\n",
    "* Choose pivot subroutine (new!)\n",
    "    * (1) break A into groups of 5, sort each group => **O(n)**\n",
    "    * (2) C = the n/5 \"middle elements\" => copying middle elements into a special array C = linear time => **O(n)**\n",
    "    * (3) p = DSelect(C, n/5, n/10) (recursively computes median of C) => **T(n) = T(n/5)**\n",
    "* (4) partition A around p => **O(n)**\n",
    "    * let j = new index of p\n",
    "* (5) if j = i return p => **O(1)**\n",
    "* (6) if j > i return RSelect(left part of A, j-1, i) \n",
    "* (7) else return RSelect(right part of A, n-j, i-j)\n",
    "    * =>  for (6) or (7) the size of an input array depends on how good the pivot is\n",
    "    * => so at the moment all we can write is **T(?)**\n",
    "    \n",
    "Let T(n) = max number of operations (=running time) of DSelect on an input array of length n\n",
    "There is a constant C >= 1 such that:\n",
    "* (1) T(1) = 1\n",
    "* T(n) =< C*n + T(n/5) + T(?)\n",
    "    * C*n – sorting groups, partitioning\n",
    "    * T(n/5) – recursive call line 3 \n",
    "    * T(?) – recursive call line (6) or (7) \n",
    "\n",
    "**Key Lemma** 2nd recursive call (in line 6 or 7) guaranteed to be on an array of size =< 7/10 * n (roughly)\n",
    "* Upshot: can replace ? by \"7/10 n\"\n",
    "* Rough Proof:\n",
    "    * Let k = n/5 = number of groups\n",
    "    * Xi = the i-th smallest of the k middle elements\n",
    "        * NB: the pivot is the final winner, it is the median of n/5 medians = n/10-th order statistic of medians array\n",
    "        * pivot = Xk/2\n",
    "    * We're trying to prove that \n",
    "        * for our proposed pivot, which is exactly this element Xk/2, we definitely get a 30-70 split or better. \n",
    "        * **Goal**: prove that >= 30% of the input array smaller than Xk/2 and >= 30% is bigger\n",
    "* Thought experiment:\n",
    "    * We lay elements of A in a 2D grid\n",
    "    * Each column: group of 5 elements in increasing order from bottom to top\n",
    "    * Rows: in 3d row – middle elements\n",
    "    * Columns arranged in increasing order of middle element [X1, X2, ..., Xk/2, ..., Xn/5]\n",
    "    * Key point: \n",
    "        * Xk/2 is bigger than 3 out of 5 (60%) of the elements in 50% of groups (lower left bart of the grid)\n",
    "        * =>  Xk/2 is bigger than 30% of A elements\n",
    "        * similarly smaller than 30% of A elements (upper right of the grid)\n",
    "* But it is not yet QED, because maybe we had to work so hard to compute the pivot that it outweighs the benefit we'd get from this guarantee. 30 70 split. \n",
    "    *  we still have to prove that we still have our linear time bound\n",
    "\n",
    "=> (by Key Lemma)\n",
    "* (1) T(1) = 1\n",
    "* T(n) =< C*n + T(n/5) + T(7/10 n) \n",
    "* Master method had an assumption that every subproblem solved is of the same size, and here it is violated\n",
    "* Strategies:\n",
    "  * (1) more general versions of the master method, of the master theorem which can accommodate a wider class of recurrences including this one here. \n",
    "  * (2) Alternatively we could push the recursion tree proof so that we could get a solution for this recurrence \n",
    "  * (3) \"hope and check\" approach: it's very ad hoc\n",
    "      * Hope: there is some constant a [independent of n], such that T(n) =< an for every n >= 1\n",
    "      * If the hope is true T(n) is bounded by an => T(n) = O(n) and algorithm is linear-time\n",
    "\n",
    "Proof by induction: \n",
    "* Claim:  We choose some constant a = 10C. C – is the constant inherited from T(n) expression\n",
    "* Then T(n) =< an for all n>=1\n",
    "* Why 10C?\n",
    "* Proof by induction on n\n",
    "    * Base case: T(1) = 1 =< a*1 (since a>=1, since c>=1)\n",
    "    * Inductive step: n>1, inductive hypothises: T(k) =< ak for every k<n\n",
    "        * We have T(n) =< C*n + T(n/5) + T(7/10 n)\n",
    "        * =< C*n + a(n/5) + a(7n/10)\n",
    "        * = n(c + 9a/10) \n",
    "        * an (by our choice of a)\n",
    "    * So claim is proved => T(n) = O(n) => algorithm runs in O(n) time\n",
    "* QED\n",
    "\n",
    "**[OMEGA(nlog(n)) LOWER BOUND FOR COMPARISON BASED ALGORITHMS]**  \n",
    "* Sorting. Can we do better then nlog(n)\n",
    "* No. Why?\n",
    "* Theorem: every \"comparison based\" sorting algorithm has worst case running time Omega(nlog(n))\n",
    "    * [assume deterministic, but lowe bound extends to randomized]\n",
    "  \n",
    "Comparison based sorting algorithm = \n",
    "* accesses the elements of the input array only via comparisons, \n",
    "* it does not do any kind of direct manipulation on a single array element. \n",
    "* All it does, is it picks pairs of elements and asks the question is the left one bigger or is the right one bigger\n",
    "* Examples: \n",
    "    * MergeSort, \n",
    "    * QuickSort, \n",
    "    * HeapSort\n",
    "* NonExamples: \n",
    "    * BucketSort – used most frequently when you have some kind of distributional assumption on the data that you're sorting\n",
    "        * Suppose you can model your data as IID (Independent and identically distributed random) samples from the uniform distribution on [0;1]\n",
    "        *  Then what you can do in bucket sort is you can just preallocate end buckets where you're gonna collect these elements. \n",
    "        * Each bucket is gonna have the same width 1/n. \n",
    "        * The first bucket you just do linear pass with the input array. \n",
    "        * Everything that's between zero and 1/n you stick in the first bucket. \n",
    "        * You classify the input elements according to which bucket they belong in\n",
    "        * And because the data is assumed to be uniform at random, you expect each of the buckets to have a very small population, just a few elements in it.\n",
    "        * And since there's N elements that means you only expect one element per bucket. \n",
    "        * Having bucketed the data, you can now just use insertion sort on each bucket independently. \n",
    "        * Because of a tiny number of elements, it'll run in constant time, \n",
    "        * and then there's gonna be linear number of buckets, so it's linear time overall. \n",
    "        * => If you're willing to make really strong assumptions about your data (= drawn uniformly at random from the interval zero one) then there's not an nlog(n) lower bound in fact you can allude the lower bound and sort them in **linear time**.\n",
    "        * Why not comparison based? it actually looks at an element at input array and it says what is its value, and according to what value it sees inside this element, it makes the decision of which bucket to allocate it to.  \n",
    "    * CountingSort\n",
    "        *  good when your data when there are small integers (between zero and K where K is say ideally at most linear in N)\n",
    "        * you do a single pass through the input array. \n",
    "        * you just bucket the elements according to what their value (from 0 to K) is in K buckets\n",
    "        * And then you do a pass, and you sort of depopulate the buckets and copy them into an output array. \n",
    "        * that gives you a sorting algorithm which runs in time = O(n)+K. Where K is the size of the biggest integer.\n",
    "        * linear time sorting\n",
    "    * RatedSort\n",
    "        * sort of an extension of counting sort\n",
    "        * you again you assume that the data are integers. \n",
    "        * You think of them in digit representation, say binary representation. \n",
    "        * And now you just sort one bit at time, starting from the least significant bits and going all the way out to the most significant bits. \n",
    "        * And so the upside of rating sort, it's an extension of counting sort is the sense that if your data is integers that are not too big, polynomially bounded in N. \n",
    "        * Then it lets you sort in linear time.\n",
    "        \n",
    "**Proof** \n",
    "* Fix a comparison-based sorting method and an array length n\n",
    "* for simplicity take an array of integers from 1 to n in some jumbled order\n",
    "* n! different potential inputs\n",
    "* Suppose algorithm always makes =< k comparisons to correctly sort these n! inputs\n",
    "* The idea behind the proof is that, \n",
    "    * because we have n! fundamentally different inputs, \n",
    "    * the sorting method has to execute in a fundamentally different way on each of those inputs. \n",
    "    * But since the only thing that causes a branch in the execution of the sorting method is the resolution of the comparison, and we have only k comparisons, \n",
    "    * <=> across all n! possible inputs algorithm exhibits =< 2^k distinct executions (ie resolutions of comparisons)\n",
    "    * So that forces 2^k >= n!. \n",
    "    * And a calculation then shows that, that forces k >= Omega(nlog(n)). \n",
    "* Pigeonhole Principle\n",
    "    * if you try to stuff K plus one pigeons into just K cubby holes, one of those K cubby holes has got to get two of the pigeons.\n",
    "    * So our pigeons = n! different inputs. \n",
    "    * our holes = different executions that the sorting method can possibly take on. \n",
    "    * Now if the number of comparisons K used is so small, that 2^K (= number of distinct ways comparisons can resolve themselves) is less than the number of different inputs that have to be correctly sorted. \n",
    "        * Then by the pigeonhole principle: two different inputs get treated in exactly the same way, by the sorting method\n",
    "        * and that means that if one comparision pattern is applied to one input and gives the correct sorting result, then the same pattern applied to another input should give an incorrect one\n",
    "    * So: since method is correct 2^k >= n! >= (n/2)^(n^2)\n",
    "    * => k >= n/2 log_2 n/2 = Omega(nlog(n))\n",
    "    \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Graphs and the Contraction Algorithm\n",
    "\n",
    "### 9.1 Graphs and Minimum Cuts\n",
    "* Further prcatice with randomized algorithms in new application domain (graphs)\n",
    "* Introduction to graphs algorithms\n",
    "* Relatively recent algorithm\n",
    "* **graph** represents pair-wise relationships among a set of objects\n",
    "    * 2 ingredients\n",
    "        * vertcies aka nodes (V) = objects\n",
    "        * edges (E) = pairs of verticies\n",
    "    * Edges can be \n",
    "        * directed = pair is ordered (arc)\n",
    "        * or underected = pair is un ordered \n",
    "* Examples: \n",
    "    * road networks\n",
    "    * the web (directed)\n",
    "    * social networks (directed and undirected)\n",
    "    * precedence constraints  \n",
    "* **Cuts** of graphs\n",
    "    * Def: a cut of a graph (V, E) is a partition of V into two non empty sets A and B\n",
    "    * Def: the crossing edges of a cut (A,B) are those with \n",
    "        * One endpoint in each of (A,B) (undirected)\n",
    "        * Edges that cross the cut from the left to the right: tail in A, head in B (directed)\n",
    "* Number if cuts for a graph with n verticies?\n",
    "    * each vertex belongs either to A or to B\n",
    "    * A and B are not null\n",
    "    * 2^n - 2\n",
    "\n",
    "**Minimum cut problem**\n",
    "* Input: an undirected graph G=(V,E) (parallel edges allowed)\n",
    "* Goal: compute a cut with fewest number of crossing edges (a min cut)\n",
    "* Applications:\n",
    "    * when your graph is representing its physical network, when identifying something like a min cut allows you to do, is identify weaknesses in your network. Or bottlenecks\n",
    "    * community detection in social networks\n",
    "    * image segmentation\n",
    "        * input = 2d array = pixels\n",
    "        * an edge between two pixels if they are neighboring: left and right or top to bottom\n",
    "        * use edge weights [(u,v) has large weight <=> you expect u,v to come from a same object]\n",
    "        * hope: repeated min cuts identifies the primary objects in pictures\n",
    "\n",
    "### 9.2 Graph representation\n",
    "* Input size has 2 parameters: (V,E) = (n,m)\n",
    "* Graph with\n",
    "    * n verticies\n",
    "    * no parallel edges (!) \n",
    "    * is connected (= is one piece)\n",
    "    * what is the minimum number of edges\n",
    "        * n-1\n",
    "    * maximum number of edges\n",
    "        * n(n-1)/2\n",
    "\n",
    "**Sparse vs Dense Graphs**\n",
    "* n = number of verticies\n",
    "* m = number of edges\n",
    "* In most (but not all) applications m is within Omega(n) and O(n^2)\n",
    "* \"sparse\" = m is O(n) or close to it\n",
    "* \"dense\" = m is closer to O(n^2)\n",
    "* this leaves ambiguity when the number of edges is something you know like n^(3/2). usually in that case you'd think of that as a dense graph. So usually anything which is more than N times logarythmic terms, you'd think of that as a dense graph\n",
    "\n",
    "**Adjacency Matrix graph representation**\n",
    "* Represent G by a n x n 0-1 matrix A, where \n",
    "    * Aij = 1 <=> G has an i-о edge\n",
    "* Variants:\n",
    "    * Aij = # of i-j edges (if parallel edges)\n",
    "    * Aij = weight of i-j edge (if any) \n",
    "    * Aij = {+1 if i->j and -1 if i<-j}\n",
    "\n",
    "**Metrics**\n",
    "* There are many metrics by which you can evaluate a data structure, or a representation. \n",
    "* two important ones \n",
    "    * the number of resources it requires and in this context, that's the amount of space that the data structure needs. \n",
    "    * The second thing is what are the operations of the data structure supports.\n",
    "    \n",
    "How much space does an adjacency matrix require, as a function of the number n of vertices and the number m of edges?\n",
    "* Theta(n^2) = it is independent of m\n",
    "    *  if you have a dense graph, the number of edges is as high as n squared, then you're not really wasting anything in this representation. \n",
    "    * But in a sparse graph, if m is much closer to linear, then this is a super wasteful representation. \n",
    "\n",
    "**Adjacency list graph representation**\n",
    "* Ingredients:\n",
    "    * array (or list) of verticies\n",
    "    * array (or list) of edges\n",
    "    * each edge points to its endpoints\n",
    "        * for directed, keeping track of heads and tails\n",
    "    * each vertex points to edges of which it is a member\n",
    "        * for directed 2 ways\n",
    "            * keep track of all of the edges, for which it is the tail\n",
    "            * If you wanted to, you can also have a second array, at a more expense of storage, where the vertex also keeps track of the edges pointing to it. (= The edges for which it's the head)\n",
    "  \n",
    "How much space does an adjacency list representation require, as a function of the number n of vertices and the number m of edges?\n",
    "* (1) array (or list) of verticies => Theta(n) space\n",
    "* (2) array (or list) of edges => Theta(m) space\n",
    "* (3) each edge points to its endpoints => Theta(m) space (because maximum 2m points) \n",
    "* (4) each vertex points to edges of which it is a member => Theta(n^2)\n",
    "    * a vertex, in principle could have edges involving all n minus 1 of the vertices => for 1 point Theta(n)\n",
    "    * you do have n vertices => that could be Theta(n^2)\n",
    "    * but! in sparse graphs n squared is way overkill to the space needed by this fourth set of pointers\n",
    "    * for each pointer in the fourth category, a vertex pointing to a given edge, there is a pointer in the third category pointing in the opposite direction, from that edge back to that vertex.\n",
    "    * there's actually a one to one correspondence between pointers in the third category, and pointers in the fourth category.\n",
    "    * => Theta(m)\n",
    "* Theta(n) + 3Theta(m) = Theta(n+m) = Theta(max(m,n)) (these are the same up to a constant factor)  \n",
    "* oftem m is bigger than n\n",
    "* but there will be a generic analysis here, which applies even if the graph is not connected.\n",
    "\n",
    "\n",
    "Question: which representation is better?  \n",
    "* It depends on the density of your graph = (on how m compares to n)\n",
    "* And it also depends on what kind of operations that you support, want to support.\n",
    "* In the cource we use Adjacency list\n",
    "    * Perfect for doing graph search\n",
    "    * adjacency matrix representation is totally out for, huge sparse graphs like the web graph\n",
    "\n",
    "\n",
    "### 9.3 Random Contraction algorithm [Karger's]\n",
    "* algorithm for computing the minimum cut of a graph.\n",
    "    * Input: an undirected graph G=(V,E) (parallel edges allowed)\n",
    "    * Goal: compute a cut with fewest number of crossing edges (a min cut) (there are exponentially possible cuts) \n",
    "* breakthrough about Karger's contraction algorithm is, it showed that random sampling can be extremely effective for fundamental graph problems\n",
    "\n",
    "**Algorithm**\n",
    "* while there are more than 2 verticies:\n",
    "   * pick a remaining edge (u,v) uniformly at random\n",
    "   * merge (or \"contract\") u and v into a single vertex\n",
    "   * remove self-loops\n",
    "* return the cut represented by 2 final verticies\n",
    "  \n",
    "this is a randomized algorithm, and randomized algorithms can behave differently on different executions. \n",
    "* and this means that this algorithm doesn't always identify the min cut\n",
    "* it depends on the random choises of edges\n",
    "\n",
    "Key question: what is a probability of success? (= output of a min cut)\n",
    "* Answer: for n^2 * ln(N) trys probability = 1-1/n\n",
    "\n",
    "**[ANALYSIS]**  \n",
    "* Fix a graph G = (V,E) with n verticies, m edges\n",
    "* Fix a minimum cut (A,B) (if there are several, fix just one of them)\n",
    "* Let k = number of edges crossing (A,B) (call these edges А)\n",
    "* What could go wrong?\n",
    "    * (1) At some iteration one of the edges А is chosen for contraction\n",
    "        * It causes that cut output by contraction algorithm would have on one side both a node from A and from B\n",
    "        * Therefore, the output of the contraction algorithm if this happens will be a different cut than (A, B)\n",
    "    * (2) let's assume now, that in each of the N-2 iterations, the contraction algorithm never contracts an edge from capital F\n",
    "         * Verticies from A are groupped together, all verticies from B are groupped together\n",
    "         * The output => desired min cut\n",
    "* Thus Pr[output is min cut (A,B)] = Pr[never contracts upon any of F edges]\n",
    "\n",
    "* Let Si = event that an edge of F contracted in iteration i\n",
    "* Goal: compute Pr[~S1 and ~S2 and ... and ~Sn-2] \n",
    "    * Pr[S1] = The probability that on the first step we select an edge from F is k/m (k – number of edges in F; m - total number of edges)  \n",
    "    * it's gonna be more useful for us to have a bound not quite as exact, an inequality. \n",
    "        * That's in terms of the number of vertices N, rather than the number of edges, M. \n",
    "        * The reason for that is, it's a little hard to understand how the number of edges is changing in each iteration\n",
    "        * One less vertex each iteration\n",
    "    * Key observation: degree (number of incident edges) of each vertex is at least k\n",
    "    * Reason: \n",
    "        * each vertex v defines a cut ({v}, V-{v})\n",
    "        * there are >= k edges in this cut (since k = number of edges in min cut)\n",
    "        * => the degree of this vertex is >= k\n",
    "    * Since sum_on_V [degree(V)] = 2m or undirected graph (sum of the degrees of all vertices is twice the number of edges)\n",
    "        * In this graph degree(V) >= k => \n",
    "        * sum_on_V [degree(V)] >= n*k \n",
    "        * => m >= k*n/2\n",
    "    * Pr[S1] = k/m =< 2/n \n",
    "        * this is a small number since n can be quite big\n",
    "        * This is good\n",
    "* Second iteration\n",
    "    * Pr[~S1 and ~S2] = Pr[~S2 | ~S1] * Pr[~S1]\n",
    "        * Pr[S1] =< 2/n\n",
    "        * Pr[~S2 | ~S1] = 1 minus the chance that we do screw up = 1 - k/(number of remaining edges) \n",
    "        * we do not have a good understanding of wow many remaining edges are there\n",
    "        * what is this number in terms of a number of remaining verticies?\n",
    "        * In the first iteration, we observed that every node in the original graph induces a cut. But the fact that's a more general statement: even after weven after we've done a bunch of contractions, any single node in the contracted graph, even if it represents a union of a bunch of nodes in the original graph, we can still think of that as a cut in the original graph\n",
    "        * this cut has also >= k edges\n",
    "        * => all degrees in contracted graph are at least k\n",
    "    * number of remaining edges >= k(n-1)/2\n",
    "    * Pr[~S2 | ~S1] = 1 - k/(number of remaining edges) >= 1-2/(n-1)\n",
    "* In general:\n",
    "    * Pr[~S1 and ~S2 and ... and ~Sn-2] = Pr[~S1] Pr[~S2|~S1] Pr[~S3|~S1 and ~S2] Pr[~Sn-2|~S1 and ... and ~Sn-3]\n",
    "    * ... >= (1-2/n)(1-2/(n-1))...(1- 2/(n-(n-4))) (1- 2/(n-(n-3)))\n",
    "    * = 2/(n(n-1)) >= 1/n^2\n",
    "* Problem: low success probability taking into account that n is big! \n",
    "    * non trivial lower bound because there are exponential number cuts in the graph. \n",
    "    * So if you try to just pick a random cut i.e you put every vertex 50:50 left or right, you'll be doing way worse than this = 2^n cuts => success probability 1/2^n\n",
    "* Solution: \n",
    "    * run the basic algorithm a large number of times N (independently)\n",
    "    * remember the smallest cut found\n",
    "* Question: **how many trials needed before we find the smallest cut?**\n",
    "    * Let Ti = event that the cut (A,B) is found on the i-th try\n",
    "    * by definition different Ti's are independent\n",
    "    * Now, we know that the, probability that a single trial fails can be pretty big, could be as big as 1-1/(N^2). But, here, now, with repeated trials, we're only in trouble if every single trial fails. If even one succeeds, then we find the meant cut\n",
    "    * Pr[all N trials fail] = Pr[~T1 and ~T2 and ... and ~TN] \n",
    "    * = product_i_1_N [Pr[~Ti]] (by independence)\n",
    "    * =< (1-1/n^2)^N\n",
    "* To answer the last question we need a fact\n",
    "    * for any real number x, 1+x =< e^x\n",
    "    * so if we take N = n^2, \n",
    "        * Pr[all fail] =< (e^(-1/n^2))^(n^2) = 1/e\n",
    "    * if we take N = n^2 * ln(n), \n",
    "        * Pr[all fail] =< (1/e)^ln(n) = 1/n\n",
    "\n",
    "We took a very simple algorithm, that almost always didn't do what we want. \n",
    "* It almost always failed to output the cut (A, B). \n",
    "* It did it with only probability 1/(n^2). \n",
    "* But, 1/(n^2) is still big enough that we can boost it, so that it almost always succeeds just by doing repeated trials. \n",
    "* And the number of repeated trials that we need is the reciprocal of its original success probability boosted by, for the logarithmic factor. \n",
    "* So that transformed this almost always failing algorithm into an almost always succeeding algorithm\n",
    "\n",
    "**Running time:**  \n",
    "* So it's way better than the exponential time you get from brute-force search through all 2^n possible cuts. \n",
    "* polynomial in n and m but slow (Omega(n^2 * m))\n",
    "    * we gotta to n^2 trials, plus a log factor, which I'm not even going to bother writing down. \n",
    "    * And also, each trial, while at the very least, you look at all the edges, so that's going to be another factor of m. \n",
    "* But can get big speed ups (to roughly O(n^2)) with more ideas\n",
    "\n",
    "\n",
    "### 9.4 Counting Minimum Cuts\n",
    "* Note: a graph can have multiple min cuts\n",
    "* Question: what is the largest number of minimum cuts that a graph with n verticies can have?\n",
    "    * at least n-1 min cuts for trees (minimual number of edges for connected graphs)\n",
    "    * at most roughly: 2^n (because this is the max num of all cuts) \n",
    "    * at most precisely: n choose 2 = n(n-1)/2\n",
    "* Proof: \n",
    "    * Lower bound:\n",
    "        * consider the n-cycle\n",
    "        * Note: each pair of the n edges defines a distinct minimum cut (with 2 crossing edges)\n",
    "        * has >=(n choose 2) min cuts\n",
    "        * no smaller value for a graph with n vertices can be an upperbound, because we can always arrange vertices in an n-cycle with max number of min cuts = n chose 2\n",
    "    * Upper bound\n",
    "        * Let (A1,B1),(A2,B2),...(At,Bt) be the min cuts of a graph with n verticies\n",
    "        * So remember how we define the success probability of a contraction algorithm. \n",
    "        * We fixed up front, some min cut (A, B). \n",
    "        * And we defined the contraction algorithm, the basic contraction algorithm, before the repeated trials. \n",
    "        * We defined the contraction algorithm as successful, if and only if it output the minimum cut (A, B) that we designated upfront. \n",
    "        * If it output some other minimum cut, we didn't count it. \n",
    "        * We said nope, that's a failure. \n",
    "        * So we actually analyzed a stronger property than what we were trying to solve, which is outputting a given min cut (A, B) rather than just any/all min cut.\n",
    "        *  let's apply it here. \n",
    "        * For each of these T minimum cuts of this graph, we can think about the probability that the contraction algorithm outputs that particular min cut. \n",
    "        * So we're gonna instantiate the analysis with a particular minimum cut (Ai, Bi). \n",
    "        * And what we proved in the analysis is that the probability that the algorithm outputs the cut (Ai, Bi), not just any/all min cut. \n",
    "        * But, in fact, this exact cut (Ai, Bi) is bounded below by 2/N(N-1), also known as 1/(N choose 2). \n",
    "        * Pr[output=(Ai, Bi) cut] >= 1/(N choose 2) = 2/N(N-1) for i = 1, ..., t\n",
    "        * Si = the event that the algorithm outputs = (Ai, Bi) cut\n",
    "        * Note Si's are disjoint events => probabilities add\n",
    "            * the probability of the union of a bunch of disjoint events is the sum of the probabilities of constituent events\n",
    "            *  the sum or probabilities of this joint events can sum to, at most, 1\n",
    "        * Thus:  \n",
    "            * there are T different min cuts for some parameter T\n",
    "            * For each min cut (Ai, Bi), a lower bound of the probability that, that could spit out as output is 1/(N choose 2).\n",
    "            * => So a lower bound on the sum of all of these probabilities is the number of them = t times the probability lower bound, 1/(N choose 2), and this has got to be at most 1\n",
    "            * ie t/(N choose 2) =< 1 \n",
    "            * => t =< (N choose 2)\n",
    "            * which is exactly the lower bound provided by the N cycle. \n",
    "            * QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4.1\n",
    "\n",
    "**1. How many different minimum cuts are there in a tree with n nodes (ie. n−1 edges)**  \n",
    "n-1\n",
    "\n",
    "**2. Let \"output\" denote the cut output by Karger's min cut algorithm on a given connected graph with n vertices, and let p = 1/{n choose 2}. Which of the following statements are true? For hints on this question, you might want to watch the short optional video on \"Counting Minimum Cuts\".**\n",
    "  \n",
    "**(a) For every graph G with n nodes and every min cut (A,B) of G,\n",
    "Pr[out=(A,B)]≥p.**  \n",
    "\n",
    "**(b) For every graph G with n nodes, there exists a min cut (A,B) such that\n",
    "Pr[out=(A,B)]≤p.**  \n",
    "\n",
    "**(c) For every graph G with n nodes and every min cut (A,B)\n",
    "Pr[out=(A,B)]≤p.**\n",
    "\n",
    "**(d) There exists a graph G with n nodes and a min cut (A,B) of G such that\n",
    "Pr[out=(A,B)]≤p.**\n",
    "\n",
    "**(e) For every graph G with n nodes, there exists a min cut (A,B) of G such that\n",
    "Pr[out=(A,B)]≥p.**  \n",
    "\n",
    "* largest number of possible min cuts on G = (V; E) (where num ov V = n) is:\n",
    "    * n chose 2 (cf 2.4)\n",
    "* probability to get the given min cut from the problem statement is = 1 / (total number of minimal cuts)\n",
    "    * total number of minimal cuts =< largest number of min cuts\n",
    "    * => probability to get the given min cut >= 1/(largest number of min cuts)\n",
    "        * = 1 / (n chose 2)\n",
    "        * = p\n",
    "    * => probability to get the given min cut >= p    \n",
    "    * that holds for any min cut\n",
    "* => (a), (e) is ture\n",
    "* (d): \n",
    "    * There is no case when Pr[out=(A,B)] < p\n",
    "    * we should think of an example when Pr[out=(A,B)] = p\n",
    "    * that is a circle, where each pair of distinct edges defines a minimum cut\n",
    "    * so it doesn't mater how would you cut, you always get the minimum\n",
    "    * number of minimum cuts is n choose 2\n",
    "    * the probability to get a chosen minimum cut is = 1/(n choose 2) = p\n",
    "    * => d is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Let 0.5<α<1 be some constant. Suppose you are looking for the median element in an array using RANDOMIZED SELECT (as explained in lectures). What is the probability that after the first iteration the size of the subarray in which the element you are looking for lies is ≤α times the size of the original array?**  \n",
    "\n",
    "* We can choose alpha differently:\n",
    "    * [————|—α-------]\n",
    "    * [————|——α----]\n",
    "    * [————|———α--]\n",
    "* We want to understand what is the probability that the medium element would be in \"small\" part of the array (which is smaller, than len(arr) * α\n",
    "    * len_small = len(arr) - len(arr) * α = n - n * α\n",
    "    * pr = len_small / len(arr) = \n",
    "    \n",
    "* RSelect takes as input an array of length n\n",
    "    * the median element should have the index i = n/2\n",
    "* RSelect chooses a pivot randomly\n",
    "* After the first partition pivot could \"land\" with equal probability  \n",
    "    * either in left side of the sorted array \n",
    "        * [––––p–––|––––––––] \n",
    "        * Xm would be in the left side subarray = Rs\n",
    "    * or in right side of sorted array\n",
    "        * [––––––––|–––p––––] \n",
    "        * Xm would be in the right side subarray = Ls\n",
    "* There is an 0.5<α<1 chosen and it can split array in 4 following ways\n",
    "    * [––X(1-α)––p–––|––––––––]\n",
    "        * this is the case when Rs =< n * α\n",
    "        * Pr[Rs =< n * α] = (0.5n -(1-α)n) / 0.5n = 2α - 1\n",
    "    * [––––p–-X(1-α)–|––––––––]\n",
    "        * this is not our case\n",
    "    * [––––––––|–X(α)––p––––] \n",
    "        * this is the case when Ls =< n * α\n",
    "        * Pr[Rs =< n * α] = (0.5n -(1-α)n) / 0.5n = 2α - 1\n",
    "    * [––––––––|–––p–X(α)–––] \n",
    "        * this is not our case\n",
    "* Pr[that subarray is =< n * α]\n",
    "    * = 0.5 Pr[Rs is =< n * α] + 0.5 Pr[Ls is =< n * α] \n",
    "    * = 2α - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Let 0<α<1 be a constant, independent of n. Consider an execution of RSelect in which you always manage to throw out at least a 1−α fraction of the remaining elements before you recurse. What is the maximum number of recursive calls you'll make before terminating?**\n",
    "  \n",
    "* we recurse till length of the input would be equal to 1\n",
    "    * level 0: len0 = n\n",
    "    * level 1 (after 1st recursive step): len1 = αn\n",
    "    * level 2: len2 = αlen1 = α^2 n\n",
    "    * level k (after k-th recursive step): lenk = α^k n\n",
    "* we will stop when lenk = α^k n = 1\n",
    "    * k = log_α(1/L) \n",
    "    * = -log_α(n) \n",
    "    * = -log(n)/log(α)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. The minimum s-t cut problem is the following. The input is an undirected graph, and two distinct vertices of the graph are labelled \"s\" and \"t\". The goal is to compute the minimum cut (i.e., fewest number of crossing edges) that satisfies the property that s and t are on different sides of the cut.\n",
    "  \n",
    "**Suppose someone gives you a subroutine for this s-t minimum cut problem via an API. Your job is to solve the original minimum cut problem (the one discussed in the lectures), when all you can do is invoke the given min s-t cut subroutine. (That is, the goal is to reduce the min cut problem to the min s-t cut problem.)**\n",
    "\n",
    "**Now suppose you are given an instance of the minimum cut problem -- that is, you are given an undirected graph (with no specially labelled vertices) and need to compute the minimum cut. What is the minimum number of times that you need to call the given min s-t cut subroutine to guarantee that you'll find a min cut of the given graph?**\n",
    "\n",
    "* to reduce the min cut problem to the min s-t cut problem\n",
    "    * we should run s-t cut on all pairs of vertices in the graph\n",
    "    * and select the minimum of all results\n",
    "* Number of pairs of points: 2 chose n\n",
    "    * this is the number of times we should run s-t cut to get all potential min cuts \n",
    "  \n",
    "But we can do better: \n",
    "* we take a = s, z = t\n",
    "    * one s-t cut run covers all cuts which separate a and z\n",
    "* only cuts that do not separate a and z are left\n",
    "    * we \"merge\" those two points in one and name it α\n",
    "    * and run s-t cut for each t in {b, ..., y}\n",
    "    * there would be n-2 runs\n",
    "* total number of runs would be n-1\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4.2 (Additional)\n",
    "\n",
    "**1. Prove that the worst-case expected running time of every randomized comparison-based sorting algorithm is Ω(nlogn). (Here the worst-case is over inputs, and the expectation is over the random coin flips made by the algorithm.)**\n",
    "  \n",
    "Proof is done in the last section of 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Suppose we modify the deterministic linear-time selection algorithm by grouping the elements into groups of 7, rather than groups of 5. (Use the \"median-of-medians\" as the pivot, as before.) Does the algorithm still run in O(n) time? What if we use groups of 3?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference would be in the second term of the expression:\n",
    "We hope that there is a constant C such that: \n",
    "T(n) =< Cn + T(n/3) + T(7/10 n)\n",
    "\n",
    "**Case with groups of 3 Proof by induction**\n",
    "* If T(n) runs in linear time there should be a constant a such as\n",
    "    * T(n) =< an for all n>=1\n",
    "* Claim: We choose some constant a = **xC**. C – is the constant inherited from T(n) expression\n",
    "    * // Then T(n) =< an for all n>=1\n",
    "    * Why 10C?\n",
    "    * Proof by induction on n\n",
    "        * Base case: T(1) = 1 =< a*1 (since a>=1, since C>=1)\n",
    "        * Inductive step: n>1, inductive hypothises: T(k) =< ak for every k<n\n",
    "        * We have T(n) =< C*n + T(n/3) + T(7/10 n)\n",
    "        * =< C*n + a(n/3) + a(7n/10)\n",
    "        * = n(C + a(10+21)/30)\n",
    "        * = n(C + a31/30)\n",
    "        * = n(a/x + a31/30)\n",
    "        * = na(1/x + 31/30)\n",
    "        * to make it = an (by our choice of a) there should be an 1/x + 31/30 = 1\n",
    "        * 1/x = -1/30\n",
    "        * x = -30\n",
    "        * a = - 30C\n",
    "        * but this violates the base case:\n",
    "            * since C>=1; a =< -30\n",
    "            * T(1) = 1 >= a*1 \n",
    "    * There is no such constant a that the base case holds => T(n) ≠ O(n) => algorithm doesn't run in O(n) time\n",
    "    \n",
    "**Case with groups of 7 Proof by induction**  \n",
    "Can be done by analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Given an array of n distinct (but unsorted) elements x_1,x_2,...x_n with positive weights w_1,w_2,...,w_n, such that sum_i_1_n [w_i] = W, a weighted median is an element x_k for which the total weight of all elements with value less than x_k (i.e., sum_x_i<x_k [w_i]) is at most W/2, and also the total weight of elements with value larger than x_k (i.e., sum_x_i>x_k [w_i]) is at most W/2. Observe that there are at most two weighted medians. Show how to compute all weighted medians in O(n) worst-case time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[not done]**\n",
    "**Case 1: there are 2 medians**\n",
    "* Let Xa[wa], Xb[wb] be 2 wheight medians\n",
    "    * (w_1 + ... + w_a-1) =< W/2   \n",
    "    * (w_1 + ... + w_a-1) + w_a + (w_a+1 ... + w_b-1) =< W/2  \n",
    "    * we sum them:\n",
    "    * 2(w1 + ... + wk1) + w_a + (w_a+1 ... + w_b-1) =< W\n",
    "        * let's express this as: 2x + w_a + y =< W\n",
    "        * as far as x =< W/2 and all w_i > 0, the inequity do hold if \n",
    "             * x < W/2\n",
    "             * y = W - 2x - w_a\n",
    "             * and in this case 2x + w_a + y = W\n",
    "    * same approach for right part\n",
    "    * (w_a+1 + ... + w_b-1) + w_b + (w_b+1 + ... + w_n) =< W/2\n",
    "    * (w_b+1 + ... + w_n) =< W/2\n",
    "    * we sum them:\n",
    "    * (w_a+1 + ... + w_b-1) + w_b + 2(w_b+1 ... + w_n) =< W\n",
    "        * let's express this as:  y + w_b + 2z =< W\n",
    "        * as far as z =< W/2 and all w_i > 0, the inequity do hold if \n",
    "             * z < W/2\n",
    "             * y = W - 2z - w_b\n",
    "             * and in this case y + w_b + 2z = W\n",
    "     * Thus 2x + w_a + W - 2z - w_b = W\n",
    "         * 2x + w_a = 2z + w_b\n",
    "         * 2(w_1 + ... + w_a-1) + w_a = 2(w_b+1 ... + w_n) + w_b\n",
    "         \n",
    "\n",
    "**You can determine the weighted median in worst case linear time as follows**\n",
    "1. If the array length is ≤2, find the weighted median by exhaustive search => **O(1)**\n",
    "2. Otherwise, \n",
    "    * find the (lower) median element X_m using the worst case 𝑂(𝑛) (deterministic) selection algorithm => **O(n)**\n",
    "    * partition the array around it (using the worst case 𝑂(𝑛) partition algorithm from QuickSort). => **O(n)**\n",
    "    * determine the weight of each part => **O(n)**\n",
    "        * If weight of the left part is <1/2 and weight of the right part is ≤1/2 then the weighted (lower) median is X_k\n",
    "        * If not, then the weighted (lower) median must necessarily lie in the partition with the larger weight\n",
    "            * So, you add the weight of the \"lighter\" part to the weight of X_k and recursively continue searching into the \"heavier\" part. \n",
    "\n",
    "T(n) = T(n/2) + O(n) => The running time of the whole algorithm is O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. We showed in an optional video lecture that every undirected graph has only polynomially (in the number n of vertices) different minimum cuts. Is this also true for directed graphs? Prove it or give a counterexample.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lower bound:\n",
    "    * n-cycle with each vertice being a tail and a a head of an edge\n",
    "    * each pair of n edges defines a distinct minimum cut (with 1 crossing edge)\n",
    "    * all cuts of this circle are min and there are n choose 2 min cuts\n",
    "* Upper bound\n",
    "    * (A1,B1),(A2,B2),...(At,Bt) be the min cuts of an undirected graph with n verticies\n",
    "    * Only some of them would be min cuts of a directed graph\n",
    "    * So the number of min cuts of directed graph would be bended above the same n chose 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. For a parameter α≥1, an α-minimum cut is one for which the number of crossing edges is at most α times that of a minimum cut. How many α-minimum cuts can an undirected graph have, as a function of α and the number n of vertices? Prove the best upper bound that you can.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper bound:\n",
    "* (A1,B1),(A2,B2),...(At,Bt) be the α-min cuts of a graph with n verticies\n",
    "* we fix upfront some α-min cut (Ai,Bi)\n",
    "* We use random contraction algorithm to output α-min cuts\n",
    "    * Let k = number of edges crossing (Ai,Bi) = α * min-cut\n",
    "    * m = no of edges \n",
    "    * n = no of vertices \n",
    "    * N = number of iterations of contraction\n",
    "    * F = {crossing edges}\n",
    "    * Thus Pr[output is α-min cut (Ai,Bi)] = Pr[never contracts upon any of F edges]\n",
    "    * Si = event that edge of F is contracted in iteration i \n",
    "    * Goal is to compute P = Pr[~S1 and ~S2 and ~S3 ... ~Sn] \n",
    "    * Pr(S1) = k/m \n",
    "    * Pr(~S1) = 1-k/m = 1 - α * min-cut/m\n",
    "    * m >= min-cut * n / 2  (see Cntraction alg analysis above)\n",
    "    * Pr(~S1) >= 1 - α * min-cut/(min-cut * n / 2)\n",
    "        * Pr(~S1) >= 1 - 2α/n\n",
    "    * Pr[~S1 and ~S2] = Pr[~S2 | ~S1] * Pr[~S1]\n",
    "        * Pr[S1] =< 2/n\n",
    "        * Pr[~S2 | ~S1] = 1 minus the chance that we do screw up = 1 - k/(number of remaining edges) = 1 - k/(n-1)\n",
    "    * P = Pr[~S1] Pr[~S2|~S1] Pr[~S3|~S1 and ~S2] Pr[~Sn-2|~S1 and ... and ~Sn-3]\n",
    "        * = (1-2α/n)(1-2α/n-1)(1-2α/n-2)..(1-2α/4)(1-2α/3)\n",
    "        * = 2α / (n(n-1)) \n",
    "        * = 1 / n ^ 2α \n",
    "    * Probability to get an exact α-min cut (Ai,Bi) = 1 / n ^ 2α \n",
    "        * max number of α-min cuts =  n ^ 2α \n",
    "    \n",
    "    \n",
    "Pr[output = (Ai, Bi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Assignment 4\n",
    "\n",
    "**The file contains the adjacency list representation of a simple undirected graph. There are 200 vertices labeled 1 to 200. The first column in the file represents the vertex label, and the particular row (other entries except the first column) tells all the vertices that the vertex is adjacent to. So for example, the 6^{th} row looks like :     \n",
    "\"6\t155\t56\t52\t120\t......\".** \n",
    "\n",
    "**This just means that the vertex with label 6 is adjacent to (i.e., shares an edge with) the vertices with labels 155,56,52,120,......,etc**\n",
    "\n",
    "**Your task is to code up and run the randomized contraction algorithm for the min cut problem and use it on the above graph to compute the min cut.** \n",
    "\n",
    "**(HINT: Note that you'll have to figure out an implementation of edge contractions. Initially, you might want to do this naively, creating a new graph from the old every time there's an edge contraction. But you should also think about more efficient implementations.)** \n",
    "\n",
    "**(WARNING: As per the video lectures, please make sure to run the algorithm many times with different random seeds, and remember the smallest cut that you ever find.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t37\\t79\\t164\\t155\\t32\\t87\\t39\\t113\\t15\\t18\\t78\\t175\\t140\\t200\\t4\\t160\\t97\\t191\\t100\\t91\\t20\\t69\\t198\\t196\\t',\n",
       " '2\\t123\\t134\\t10\\t141\\t13\\t12\\t43\\t47\\t3\\t177\\t101\\t179\\t77\\t182\\t117\\t116\\t36\\t103\\t51\\t154\\t162\\t128\\t30\\t']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../algorithms_course_code/data/pg_asmt_4_mincut.txt', 'r') as f:\n",
    "    x = f.read().splitlines()\n",
    "    f.close()\n",
    "x[0:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../algorithms_course_code/data/pg_asmt_4_mincut.txt', 'r')\n",
    "graph = {}\n",
    "for line in file:\n",
    "    line_split = line.split(\"\\t\")[:-1]\n",
    "    key = int(line_split[0])\n",
    "    line_split_int_values = [int(i) for i in line_split[1:]]\n",
    "    graph[key] = line_split_int_values\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of one minimal cut naive**\n",
    "define an edge\n",
    "0. check if len(graph) > 2\n",
    "1. define a random number from 0 to len(graph) = x\n",
    "2. pick vertix u under this number\n",
    "3. define a random number from 0 to len(of adjacent vertices row) = y\n",
    "4. pick vertix v under this number\n",
    "merge u and v\n",
    "5. merge u and v into w at position x\n",
    "6. delete x from w\n",
    "7. delete y from w\n",
    "8. replace y in all other vertices by x \n",
    "recursion\n",
    "9. recure till len(graph) > 2\n",
    "10. return number of edges = len(u) = len(v) \n",
    "randomness\n",
    "11. repeat n^2 * ln(n) times to be sure get a min answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def loadGraph():\n",
    "    file = open(\"../algorithms_course_code/data/pg_asmt_4_mincut_test_cases.txt\", 'r')\n",
    "    graph = {}\n",
    "    for line in file:\n",
    "        line_split = line.split(\" \")\n",
    "        line_split_int = [int(i) for i in line_split]\n",
    "        key = line_split_int[0]\n",
    "        values =  line_split_int[1:]\n",
    "        graph[key] = values\n",
    "    file.close()    \n",
    "    return graph\n",
    "\n",
    "def loadGraph2():\n",
    "    file = open('../algorithms_course_code/data/pg_asmt_4_mincut.txt', 'r')\n",
    "    graph = {}\n",
    "    for line in file:\n",
    "        line_split = line.split(\"\\t\")[:-1]\n",
    "        key = int(line_split[0])\n",
    "        line_split_int_values = [int(i) for i in line_split[1:]]\n",
    "        graph[key] = line_split_int_values\n",
    "    file.close()    \n",
    "    return graph\n",
    "\n",
    "def chooseRandomVerticies(graph):\n",
    "    v1 = random.randint(0, len(graph)-1)\n",
    "    v1_id = list(graph.keys())[v1]\n",
    "    v2 = random.randint(0, len(graph[v1_id])-1)\n",
    "    v2_id = graph[v1_id][v2]\n",
    "    return v1_id, graph[v1_id], v2_id, graph[v2_id]\n",
    "\n",
    "def mergeVertices(v1_id, v1_g, v2_id, v2_g):\n",
    "    # merge virtix with larger index into the one with smaller\n",
    "    if v1_id < v2_id:\n",
    "        vm_id = v1_id\n",
    "        vm_g_raw = v1_g + v2_g\n",
    "    else:\n",
    "        vm_id = v2_id\n",
    "        vm_g_raw = v2_g + v1_g        \n",
    "    \n",
    "    # remove self referring\n",
    "    vm_g = [v for v in vm_g_raw if (v != v1_id and v != v2_id)]\n",
    "    return vm_id, vm_g\n",
    "\n",
    "def oneCut(graph):    \n",
    "    while len(graph) > 2:\n",
    "        v1_id, v1_g, v2_id, v2_g = chooseRandomVerticies(graph)\n",
    "        vm_id, vm_g = mergeVertices(v1_id, v1_g, v2_id, v2_g)\n",
    "    \n",
    "        # delete vertix with bigger index\n",
    "        # replace list of vertix with smaller index by vm_g\n",
    "        if v1_id < v2_id:\n",
    "            del graph[v2_id]\n",
    "            graph[v1_id] = vm_g\n",
    "        else:\n",
    "            del graph[v1_id]\n",
    "            graph[v2_id] = vm_g    \n",
    "    \n",
    "        #replace all v2_id in other vertices lists by vm_id\n",
    "        for key in list(graph.keys()):\n",
    "            for i in range(0, len(graph[key])):\n",
    "                if (v1_id < v2_id and graph[key][i] == v2_id):\n",
    "                    graph[key][i] = vm_id \n",
    "                elif (v1_id > v2_id and graph[key][i] == v1_id): \n",
    "                    graph[key][i] = vm_id \n",
    "        # recursion \n",
    "        oneCut(graph)\n",
    "        \n",
    "    first_key = list(graph.keys())[0]\n",
    "    return len(graph[first_key])\n",
    "\n",
    "# def probaMinCut(graph):   \n",
    "# for improvements !!! see: \n",
    "# https://www.coursera.org/learn/algorithms-divide-conquer/discussions/weeks/4/threads/rStE-lz7EeeMcg4GWtvWkg\n",
    "# https://www.coursera.org/learn/algorithms-divide-conquer/discussions/weeks/4/threads/Y8zZUBM1TyaM2VATNQ8mog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 120, 21, 21, 17, 48, 107, 22, 115, 150]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "min_cuts = []\n",
    "\n",
    "g = loadGraph2()\n",
    "n = len(g.keys())\n",
    "i = 0 \n",
    "\n",
    "while i < ((n**2) * np.log(n)): # !!! that takes tooo much time. n*np.log(n) more than enough ~ 1070 runs\n",
    "    graph = loadGraph2()\n",
    "    min_cut = oneCut(graph)\n",
    "    min_cuts.append(min_cut)\n",
    "    i += 1\n",
    "\n",
    "print(min_cuts[:10])\n",
    "print (min(min_cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 / 1071\n"
     ]
    }
   ],
   "source": [
    "print(min_cuts.count(17), \"/\", len(min_cuts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Exam\n",
    "\n",
    "**1. Recall the Partition subroutine that we used in both QuickSort and RSelect. Suppose that the following array has just been partitioned around some pivot element: 3, 1, 2, 4, 5, 8, 7, 6, 9**\n",
    "**Which of these elements could have been the pivot element? (Hint: Check all that apply, there could be more than one possibility!)**\n",
    "\n",
    "4 and 5\n",
    "  \n",
    "   \n",
    "**2. Here is an array of ten integers: 5 3 8 9 1 7 0 2 6 4**\n",
    "**Suppose we run MergeSort on this array. What is the number in the 7th position of the partially sorted array after the outermost two recursive calls have completed (i.e., just before the very last Merge step)? (When we say \"7th\" position, we're counting positions starting at 1; for example, the input array has a \"0\" in its 7th position.)**\n",
    "\n",
    "I used algorithm which I coded on first week   \n",
    "[5, 3, 8, 9, 1, 7, 0, 2, 6, 4]\n",
    "  \n",
    "first half: devide  \n",
    "[5, 3, 8, 9, 1]   \n",
    "[5, 3]  [8, 9, 1]    \n",
    "[5]  [3]  [8]  [9, 1]  \n",
    "[5]  [3]  [8]  [9]  [1]  \n",
    "  \n",
    "first half: merge\n",
    "[3, 5]  [8]  [1, 9]  \n",
    "[3, 5]  [1, 8, 9]  \n",
    "[1, 3, 5, 8, 9]  \n",
    "  \n",
    "second half: devide  \n",
    "[7, 0, 2, 6, 4]  \n",
    "[7, 0]  [2, 6, 4]  \n",
    "[7]  [0]  [2]  [6, 4] \n",
    "[7]  [0]  [2]  [6]  [4]  \n",
    "  \n",
    "second half: merge\n",
    "[0, 7]  [2]  [4, 6]   \n",
    "[0, 7]  [2, 4, 6]   \n",
    "[0, 2, 4, 6, 7]  \n",
    "  \n",
    "two unmerged but sorted halves  \n",
    "[1, 3, 5, 8, 9]  [0, 2, 4, 6, 7]    \n",
    "\n",
    "seventh position is 2\n",
    "  \n",
    "   \n",
    "**3. What is the asymptotic worst-case running time of MergeSort, as a function of the input array length n**\n",
    "  \n",
    "Theta(nlon(n))\n",
    "  \n",
    "    \n",
    "**4. What is the asymptotic running time of Randomized QuickSort on arrays of length n, in expectation (over the choice of random pivots) and in the worst case, respectively?**\n",
    "    \n",
    "Θ(nlogn) [expected] and Θ(n^2) [worst case]\n",
    "\n",
    "\n",
    "**5. Let f and g be two increasing functions, defined on the natural numbers, with f(1),g(1) ≥1. Assume that f(n)=O(g(n)).** \n",
    "**Is 2^(f(n)) = O(2^(g(n)))? (Multiple answers may be correct, check all that apply.)**\n",
    "\n",
    "* f(n)=O(g(n)) <=> \n",
    "    * there exist 2 constants c1, n1_0 both > 0, \n",
    "    * such that f(n) =< c1*g(n) for all n >= n1_0\n",
    "* 2^(f(n)) = O(2^(g(n))) <=>\n",
    "    * there exist 2 constants c2, n2_0 both > 0\n",
    "        * we take c2 = 1; n2_0 = n1_0 + 1\n",
    "        * then 2^(f(n)) =< c2*2^(g(n)) = 2^(g(n))\n",
    "        * log_2(2^(f(n))) =< log_2(2^(g(n)))\n",
    "        * f(n) =< g(n) \n",
    "\n",
    " \n",
    "– Yes if f(n)≤g(n) for all sufficiently large nn\n",
    "– Maybe, maybe not (depends on the functions f and g).\n",
    "  \n",
    "  \n",
    "**6. Let 0<α<.5 be some constant. Consider running the Partition subroutine on an array with no duplicate elements and with the pivot element chosen uniformly at random (as in QuickSort and RSelect). What is the probability that, after partitioning, both subarrays (elements to the left of the pivot, and elements to the right of the pivot) have size at least α times that of the original array?**  \n",
    "\n",
    "* Array A of length n\n",
    "* [--------αn---|---(1-α)n------]\n",
    "* to have both subarrays >= αn pivot must be chosen within αn-th and (1-α)n-th elements of sorted array A\n",
    "* Since pivot is chosen uniformly at random\n",
    "    * Pr[pivot = Xi] = 1/n\n",
    "    * the probability it fall into a specific part of an array would be the len(part)/len(array)\n",
    "* Pr[len[left] >= αn and [len[right] >= αn] = [(1-α)n - αn] / n\n",
    "    * = 1-2α\n",
    "Pr[pivot = Xi] = 1/n\n",
    "Pr[len[B] >= α n] = (n/2 - α n) / (n/2) = 1-2α\n",
    "\n",
    "**7. Suppose that a randomized algorithm succeeds (e.g., correctly computes the minimum cut of a graph) with probability p (with 0<p<1). Let ϵ be a small positive number (less than 1).**\n",
    "**How many independent times do you need to run the algorithm to ensure that, with probability at least 1−ϵ, at least one trial succeeds?**\n",
    "  \n",
    "* Pr[1 run of algorithm outputs min cut] = 1/n^2 = p\n",
    "* Pr [N runs output at least one min cut] = 1 - (1 - 1/n^2)^N = 1 - (1 - p)^N\n",
    "* Find N such as \n",
    "    * 1 - (1 - p)^N = 1 - ϵ\n",
    "    * (1 - p)^N = ϵ\n",
    "    * N = log_1-p(ϵ)\n",
    "    * N = log(ϵ)/log(1-p)\n",
    "    \n",
    "      \n",
    "**8. Suppose you are given k sorted arrays, each with n elements, and you want to combine them into a single array of kn elements. Consider the following approach. Divide the k arrays into k/2 pairs of arrays, and use the Merge subroutine taught in the MergeSort lectures to combine each pair. Now you are left with k/2 sorted arrays, each with 2n elements. Repeat this approach until you have a single sorted array with kn elements. What is the running time of this procedure, as a function of k and n?**\n",
    "\n",
    "* level 0: runs=0, len=n, arr_num=k\n",
    "* level 1: runs=1, len=2n, arr_num=k/2\n",
    "* level 2: runs=2, len=4n, arr_num=k/4\n",
    "* level j: runs=j, len=2^j * n, arr_num=k/2^j\n",
    "* last level log_2(k): runs=log_2(k), len=k * n, arr_num=1\n",
    "  \n",
    "* number of operations on a level =< 2^j * n * k/2^j = n * k\n",
    "* number of operations on all levels =< k * n * log_2(k)\n",
    "\n",
    "=> Theta(n k log(k))\n",
    "\n",
    "**9.Running time of Strassen's matrix multiplication algorithm: Suppose that the running time of an algorithm is governed by the recurrence T(n)=7∗T(n/2)+n^2. What's the overall asymptotic running time (i.e., the value of T(n))?**\n",
    "\n",
    "* Master method: T(n) =< a*T(n/b) + O(n^d)\n",
    "    * T(n)=< 7∗T(n/2) + O(n^2)\n",
    "    * a = 7\n",
    "    * b = 2\n",
    "    * d = 2\n",
    "    * a = 7 > b^d = 4\n",
    "    * T(n) = O(n^(log_2(7)) \n",
    "    \n",
    "**10. Recall the Master Method and its three parameters a,b,d. Which of the following is the best interpretation of b^d, in the context of divide-and-conquer algorithms?**\n",
    "  \n",
    "**(a) The rate at which the work-per-subproblem is shrinking (per level of recursion).**  \n",
    "**(b) The rate at which the total work is growing (per level of recursion).**  \n",
    "**(c) The rate at which the number of subproblems is growing (per level of recursion).**  \n",
    "**(d) The rate at which the subproblem size is shrinking (per level of recursion).**  \n",
    "\n",
    "The answer is \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"For 9 years I have been working as an information architect. Since last four years I was working a lot within engineering team on the construction of a data processing pipeline which fed clean structured data to some text\"\n",
    "b = a.split(\" \")\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
